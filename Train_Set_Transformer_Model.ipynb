{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyabratkumarsingh/option-portfolio-encoder-decoder/blob/main/Train_Set_Transformer_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJZFIbU9_YfV",
        "outputId": "b2ec5333-df5f-47db-eb3b-c5d0873a9638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.12/dist-packages (3.53.0)\n",
            "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (0.24.1)\n",
            "Requirement already satisfied: everett<3.2.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet_ml) (3.1.0)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (4.25.1)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (5.9.5)\n",
            "Requirement already satisfied: python-box<7.0.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (6.1.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.32.4)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (13.9.4)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.38.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from comet_ml) (75.2.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.12/dist-packages (from comet_ml) (3.20.1)\n",
            "Requirement already satisfied: urllib3>=1.26.8 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (1.17.3)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (3.1.1)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.12/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet_ml) (5.0.9)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.27.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->comet_ml) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->comet_ml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->comet_ml) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.3.2->comet_ml) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.3.2->comet_ml) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema!=3.1.0,>=2.6.0->comet_ml) (4.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install comet_ml\n",
        "!pip install tqdm\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5A1HHxp_kiG",
        "outputId": "dd86aaf5-c303-43a1-f17f-fad1405e191c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "fNUnLv8l_pSQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def delete_file_from_drive(full_file_path):\n",
        "  if os.path.exists(full_file_path):\n",
        "      try:\n",
        "          os.remove(full_file_path)\n",
        "          print(f\"File '{full_file_path}' successfully deleted from Google Drive.\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error deleting file '{full_file_path}': {e}\")\n",
        "  else:\n",
        "      print(f\"File '{full_file_path}' not found at '{full_file_path}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbPrPqqxhv1s",
        "outputId": "5c387a37-8fd0-4cc0-bdf4-c17e50145d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibrated SP500 1Y Volatility (Sigma): 0.1932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import itertools\n",
        "from itertools import product\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc # For garbage collection\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "\n",
        "\n",
        "# Parameters\n",
        "MU = 0.05\n",
        "T = 1.0 # Time to maturity\n",
        "NOISE_STD = 0.005\n",
        "MIN_PRICE_RANGE = 100\n",
        "MAX_PRICE_RANGE = 500\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def calibrate_sigma_from_sp500(period=\"1y\", ticker=\"SPY\"):\n",
        "    \"\"\"\n",
        "    Calibrate volatility (sigma) using S&P500 proxy (SPY ETF).\n",
        "    Computes realized annualized volatility from historical data.\n",
        "\n",
        "    Args:\n",
        "        period (str): Period to download (e.g. \"1y\", \"2y\").\n",
        "        ticker (str): Symbol to use, default SPY (ETF for S&P 500).\n",
        "\n",
        "    Returns:\n",
        "        float: Annualized volatility (sigma).\n",
        "    \"\"\"\n",
        "    # Download daily adjusted close prices\n",
        "    data = yf.download(ticker, period=period, interval=\"1d\", auto_adjust=True)\n",
        "\n",
        "    closes = data[\"Close\"].dropna()\n",
        "\n",
        "    # Compute log returns\n",
        "    log_returns = np.log(closes / closes.shift(1)).dropna()\n",
        "\n",
        "    # Daily volatility\n",
        "    sigma_daily = log_returns.std()\n",
        "\n",
        "    # Annualized volatility\n",
        "    sigma_annual = sigma_daily * np.sqrt(252)\n",
        "\n",
        "    # Convert safely to float\n",
        "    return sigma_annual.item() if hasattr(sigma_annual, \"item\") else float(sigma_annual)\n",
        "\n",
        "# Example: update SIGMA\n",
        "SIGMA = calibrate_sigma_from_sp500()\n",
        "print(f\"Calibrated SP500 1Y Volatility (Sigma): {SIGMA:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g8r9orFZ_sLT"
      },
      "outputs": [],
      "source": [
        "\n",
        "TFR_RATIO = 0.5\n",
        "\n",
        "def generate_hybrid_strikes(S_0, option_types, rng, training_friendly_ratio=TFR_RATIO):\n",
        "    n = len(option_types)\n",
        "    K_prices = np.zeros(n, dtype=np.float32)\n",
        "\n",
        "    for i in range(n):\n",
        "        if rng.random() < training_friendly_ratio:  # 40% chance\n",
        "            K_prices[i] = S_0 * rng.uniform(0.90, 1.20)  # Mixed ITM/OTM\n",
        "        else:\n",
        "            # Realistic strikes\n",
        "            if option_types[i] == \"call\":\n",
        "                K_prices[i] = S_0 * rng.uniform(0.95, 1.15)  # Mix of ITM/OTM calls\n",
        "            else:\n",
        "                K_prices[i] = S_0 * rng.uniform(0.85, 1.05)  # Mix of ITM/OTM puts\n",
        "\n",
        "    return K_prices\n",
        "\n",
        "\n",
        "def generate_option_prices_for_idx(idx, n, weights=None, training_friendly_ratio=TFR_RATIO):\n",
        "\n",
        "    rng = np.random.default_rng(idx)\n",
        "\n",
        "    torch.manual_seed(idx)\n",
        "    if DEVICE.type == 'cuda':\n",
        "        torch.cuda.manual_seed_all(idx)\n",
        "\n",
        "    S_0 = rng.uniform(MIN_PRICE_RANGE, MAX_PRICE_RANGE)\n",
        "\n",
        "    # Generate option types\n",
        "    option_types = rng.choice([\"call\", \"put\"], size=n)\n",
        "    option_types_numeric = np.where(option_types == \"call\", 1, 0).astype(np.float32)\n",
        "\n",
        "    # Generate hybrid strike prices\n",
        "    K_prices = generate_hybrid_strikes(S_0, option_types, rng, TFR_RATIO)\n",
        "\n",
        "    # Generate or use weights\n",
        "    if weights is None:\n",
        "        weight_sets = generate_combinatorial_weights_manageable(n)\n",
        "        weights_array = weight_sets[0]\n",
        "    else:\n",
        "        weights_array = np.array(weights, dtype=np.float32)\n",
        "\n",
        "    return K_prices, option_types_numeric, S_0, weights_array\n",
        "\n",
        "\n",
        "\n",
        "def generate_combinatorial_weights_manageable(n, base_weights=[-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75]):\n",
        "    weight_sets = []\n",
        "\n",
        "    # Handle the case where n < 2\n",
        "    if n < 2:\n",
        "        weights = np.zeros(n, dtype=np.float32)\n",
        "        if n == 1:\n",
        "            # If only one position, assign a long position (1.0)\n",
        "            weights[0] = 1.0\n",
        "        weight_sets.append(weights)\n",
        "        return weight_sets\n",
        "\n",
        "    # Generate a single portfolio: either one long or one short, and the rest from combinatorics\n",
        "    weights = np.zeros(n, dtype=np.float32)\n",
        "\n",
        "    # Randomly choose if we want a long or short portfolio\n",
        "    is_long = random.choice([True, False])\n",
        "\n",
        "    if is_long:\n",
        "        # Choose one position to be long (1.0)\n",
        "        long_idx = random.randint(0, n - 1)\n",
        "        weights[long_idx] = 1.0\n",
        "    else:\n",
        "        # Choose one position to be short (-1.0)\n",
        "        short_idx = random.randint(0, n - 1)\n",
        "        weights[short_idx] = -1.0\n",
        "\n",
        "    # Fill remaining positions with combinatorial weights from base_weights\n",
        "    remaining_positions = [i for i in range(n) if weights[i] == 0]  # Find positions not yet filled\n",
        "    combinatorics = np.random.choice(base_weights, size=len(remaining_positions), replace=True)\n",
        "\n",
        "    # Assign combinatorial weights to the remaining positions without normalization\n",
        "    weights[remaining_positions] = combinatorics\n",
        "\n",
        "    weight_sets.append(weights)\n",
        "\n",
        "    return weight_sets\n",
        "\n",
        "def compute_cashflow_delta(S_T, portfolio):\n",
        "    \"\"\"\n",
        "    Computes the delta of each option in the portfolio w.r.t terminal stock price S_T.\n",
        "\n",
        "    Args:\n",
        "        S_T (Tensor): [B] or [B, 1] terminal stock prices\n",
        "        portfolio (Tensor): [B, N, 3] with columns (K, type, weight)\n",
        "\n",
        "    Returns:\n",
        "        Tensor: [B, N] delta for each option\n",
        "    \"\"\"\n",
        "    # FIXED: Ensure S_T has correct shape\n",
        "    if S_T.dim() > 2:\n",
        "        S_T = S_T.squeeze()  # Remove extra dimensions\n",
        "    if S_T.dim() == 2 and S_T.shape[1] == 1:\n",
        "        S_T = S_T.squeeze(-1)  # [B, 1] → [B]\n",
        "\n",
        "    strikes = portfolio[..., 0]  # [B, N]\n",
        "    types = portfolio[..., 1]    # [B, N]\n",
        "\n",
        "    # FIXED: Expand S_T to match portfolio shape [B, N]\n",
        "    if S_T.dim() == 1:\n",
        "        S_T_exp = S_T.unsqueeze(-1).expand(-1, strikes.shape[1])  # [B] → [B, N]\n",
        "    else:\n",
        "        S_T_exp = S_T.expand(-1, strikes.shape[1])  # [B, 1] → [B, N]\n",
        "\n",
        "    # Call delta: 1 if S_T > K\n",
        "    call_delta = ((types == 1) & (S_T_exp > strikes)).float()\n",
        "    # Put delta: -1 if S_T < K\n",
        "    put_delta = -((types == 0) & (S_T_exp < strikes)).float()\n",
        "\n",
        "    delta_each = call_delta + put_delta  # [B, N]\n",
        "    return delta_each\n",
        "\n",
        "def compute_cashflow(portfolio, S_T):\n",
        "    \"\"\"\n",
        "    Compute cashflow and portfolio-level derivative for a batch of portfolios.\n",
        "\n",
        "    Args:\n",
        "        portfolio (Tensor): [B, N, 3] tensor with (K, type, weight)\n",
        "        S_T (Tensor): [B] or [B, 1] terminal stock prices\n",
        "\n",
        "    Returns:\n",
        "        cashflow (Tensor): [B] total weighted payoff per portfolio\n",
        "        derivative (Tensor): [B] portfolio delta (sum of weighted option deltas)\n",
        "    \"\"\"\n",
        "    # FIXED: Ensure S_T has correct shape\n",
        "    if S_T.dim() > 2:\n",
        "        S_T = S_T.squeeze()  # Remove extra dimensions\n",
        "    if S_T.dim() == 2 and S_T.shape[1] == 1:\n",
        "        S_T = S_T.squeeze(-1)  # [B, 1] → [B]\n",
        "\n",
        "    strikes = portfolio[..., 0]  # [B, N]\n",
        "    types = portfolio[..., 1]    # [B, N]\n",
        "    weights = portfolio[..., 2]  # [B, N]\n",
        "\n",
        "    # FIXED: Expand S_T to match portfolio shape [B, N]\n",
        "    if S_T.dim() == 1:\n",
        "        S_T_exp = S_T.unsqueeze(-1).expand(-1, strikes.shape[1])  # [B] → [B, N]\n",
        "    else:\n",
        "        S_T_exp = S_T.expand(-1, strikes.shape[1])  # [B, 1] → [B, N]\n",
        "\n",
        "    # Compute payoffs\n",
        "    call_payoffs = torch.relu(S_T_exp - strikes)  # [B, N]\n",
        "    put_payoffs = torch.relu(strikes - S_T_exp)   # [B, N]\n",
        "    payoffs = torch.where(types == 1, call_payoffs, put_payoffs)  # [B, N]\n",
        "\n",
        "    # Weighted cashflow\n",
        "    weighted_payoffs = payoffs * weights  # [B, N]\n",
        "    cashflow = weighted_payoffs.sum(dim=1)  # [B]\n",
        "\n",
        "    # Compute derivative (delta per option)\n",
        "    delta_each = compute_cashflow_delta(S_T, portfolio)  # [B, N]\n",
        "    weighted_delta = delta_each * weights  # [B, N]\n",
        "\n",
        "    # Portfolio-level derivative\n",
        "    derivative = weighted_delta.sum(dim=1)  # [B]\n",
        "\n",
        "    return cashflow.float(), derivative.float()\n",
        "\n",
        "def compute_cashflow_vectorized(portfolio, S_T_batch):\n",
        "    \"\"\"\n",
        "    Vectorized computation for multiple S_T scenarios.\n",
        "\n",
        "    Args:\n",
        "        portfolio (Tensor): [B, N, 3] tensor with (K, type, weight)\n",
        "        S_T_batch (Tensor): [B, M] multiple terminal prices per portfolio\n",
        "\n",
        "    Returns:\n",
        "        cashflow (Tensor): [B, M] cashflows for each scenario\n",
        "        derivative (Tensor): [B, M] derivatives for each scenario\n",
        "    \"\"\"\n",
        "    B, N, _ = portfolio.shape\n",
        "    B2, M = S_T_batch.shape\n",
        "    assert B == B2, f\"Batch size mismatch: portfolio {B}, S_T {B2}\"\n",
        "\n",
        "    # Expand tensors for vectorized computation\n",
        "    portfolio_exp = portfolio.unsqueeze(2).expand(-1, -1, M, -1)  # [B, N, M, 3]\n",
        "    S_T_exp = S_T_batch.unsqueeze(1).expand(-1, N, -1)  # [B, N, M]\n",
        "\n",
        "    strikes = portfolio_exp[..., 0]  # [B, N, M]\n",
        "    types = portfolio_exp[..., 1]    # [B, N, M]\n",
        "    weights = portfolio_exp[..., 2]  # [B, N, M]\n",
        "\n",
        "    # Compute payoffs for all scenarios\n",
        "    call_payoffs = torch.relu(S_T_exp - strikes)  # [B, N, M]\n",
        "    put_payoffs = torch.relu(strikes - S_T_exp)   # [B, N, M]\n",
        "    payoffs = torch.where(types == 1, call_payoffs, put_payoffs)  # [B, N, M]\n",
        "\n",
        "    # Compute deltas for all scenarios\n",
        "    call_delta = ((types == 1) & (S_T_exp > strikes)).float()  # [B, N, M]\n",
        "    put_delta = -((types == 0) & (S_T_exp < strikes)).float()  # [B, N, M]\n",
        "    delta_each = call_delta + put_delta  # [B, N, M]\n",
        "\n",
        "    # Aggregate across options (sum over N dimension)\n",
        "    cashflow = (payoffs * weights).sum(dim=1)  # [B, M]\n",
        "    derivative = (delta_each * weights).sum(dim=1)  # [B, M]\n",
        "\n",
        "    return cashflow.float(), derivative.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5uf3nsG7_0MK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class OperatorDatasetStandardized(Dataset):\n",
        "    def __init__(self, num_samples, num_samples_S_T,\n",
        "                 K_scaler=None, S_T_scaler=None, cashflow_scaler=None,\n",
        "                 is_fitting_mode=False, max_portfolio_size=100, min_portfolio_size=1):\n",
        "        self.num_samples = num_samples\n",
        "        self.num_samples_S_T = num_samples_S_T\n",
        "        self.max_portfolio_size = max_portfolio_size\n",
        "        self.min_portfolio_size = min_portfolio_size\n",
        "        self.is_fitting_mode = is_fitting_mode\n",
        "\n",
        "        if not is_fitting_mode and any(s is None for s in (K_scaler, S_T_scaler, cashflow_scaler)):\n",
        "            raise ValueError(\"K_scaler, S_T_scaler, and cashflow_scaler must be provided in evaluation mode.\")\n",
        "\n",
        "        self.K_scaler = K_scaler\n",
        "        self.S_T_scaler = S_T_scaler\n",
        "        self.cashflow_scaler = cashflow_scaler\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def _simulate_terminal_prices(self, S_0):\n",
        "        # Example: Geometric Brownian Motion with noise\n",
        "        Z = torch.clamp(torch.randn(self.num_samples_S_T), -3, 3)\n",
        "        drift = (MU - 0.5 * SIGMA**2) * T\n",
        "        diffusion = SIGMA * torch.sqrt(torch.tensor(T, dtype=torch.float32))\n",
        "        S_T = S_0 * torch.exp(drift + diffusion * Z)\n",
        "        S_T += torch.randn_like(S_T) * (NOISE_STD * S_T)\n",
        "        return S_T.float()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        portfolio_len = np.random.randint(self.min_portfolio_size, self.max_portfolio_size + 1)\n",
        "        K, option_types, S_0, weights = generate_option_prices_for_idx(idx, portfolio_len)\n",
        "\n",
        "        # Ensure float32\n",
        "        K = torch.tensor(K, dtype=torch.float32)\n",
        "        option_types = torch.tensor(option_types, dtype=torch.float32)\n",
        "        weights = torch.tensor(weights, dtype=torch.float32)\n",
        "        S_0 = torch.tensor(S_0, dtype=torch.float32)\n",
        "\n",
        "        # Portfolio tensor\n",
        "        portfolio = torch.stack([K, option_types, weights], dim=-1)\n",
        "        pad_len = self.max_portfolio_size - portfolio_len\n",
        "        if pad_len > 0:\n",
        "            pad_tensor = torch.zeros(pad_len, 3, dtype=torch.float32)\n",
        "            portfolio = torch.cat([portfolio, pad_tensor], dim=0)\n",
        "\n",
        "        mask = torch.tensor([True]*portfolio_len + [False]*pad_len, dtype=torch.bool)\n",
        "\n",
        "        # Terminal prices - shape [num_samples_S_T]\n",
        "        S_T = self._simulate_terminal_prices(S_0)\n",
        "\n",
        "        # Store denormalized versions\n",
        "        portfolio_denorm = portfolio.clone()\n",
        "        S_T_denorm = S_T.clone()\n",
        "\n",
        "        # FIXED: Use vectorized computation\n",
        "        # Expand portfolio for batch computation: [1, max_portfolio_size, 3]\n",
        "        portfolio_batch = portfolio_denorm.unsqueeze(0)  # Add batch dimension\n",
        "        # Expand S_T for batch computation: [1, num_samples_S_T]\n",
        "        S_T_batch = S_T_denorm.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Use vectorized computation\n",
        "        cashflow_denorm, derivative_denorm = compute_cashflow_vectorized(\n",
        "            portfolio_batch, S_T_batch\n",
        "        )\n",
        "\n",
        "        # Remove batch dimension: [1, M] → [M]\n",
        "        cashflow_denorm = cashflow_denorm.squeeze(0).float()\n",
        "        derivative_denorm = derivative_denorm.squeeze(0).float()\n",
        "\n",
        "        if not self.is_fitting_mode:\n",
        "            # Normalize only valid options\n",
        "            valid_len = mask.sum().item()\n",
        "            if valid_len > 0:\n",
        "                K_norm = torch.tensor(\n",
        "                    self.K_scaler.transform(K[:valid_len].unsqueeze(1)),\n",
        "                    dtype=torch.float32\n",
        "                ).squeeze()\n",
        "                portfolio[:valid_len, 0] = K_norm\n",
        "\n",
        "            # Normalize S_T: [M] → [M, 1] → transform → [M]\n",
        "            S_T = torch.tensor(\n",
        "                self.S_T_scaler.transform(S_T_denorm.unsqueeze(1)),\n",
        "                dtype=torch.float32\n",
        "            ).squeeze()\n",
        "\n",
        "            # Normalize cashflow: [M] → [M, 1] → transform → [M]\n",
        "            cashflow = torch.tensor(\n",
        "                self.cashflow_scaler.transform(cashflow_denorm.unsqueeze(1)),\n",
        "                dtype=torch.float32\n",
        "            ).squeeze()\n",
        "\n",
        "            # Scale derivative to match cashflow std\n",
        "            derivative = derivative_denorm.clone()\n",
        "            cf_std = cashflow.std().item()\n",
        "            deriv_std = derivative.std().item()\n",
        "            if deriv_std > 0:\n",
        "                derivative = derivative * (cf_std / deriv_std)\n",
        "        else:\n",
        "            cashflow = cashflow_denorm.clone()\n",
        "            derivative = derivative_denorm.clone()\n",
        "            S_T = S_T_denorm.clone()\n",
        "\n",
        "        return {\n",
        "            \"portfolio\": portfolio.float(),          # [max_portfolio_size, 3]\n",
        "            \"mask\": mask,                            # [max_portfolio_size]\n",
        "            \"S_T\": S_T.float(),                      # [num_samples_S_T]\n",
        "            \"cashflow\": cashflow.float(),            # [num_samples_S_T]\n",
        "            \"derivative\": derivative.float(),        # [num_samples_S_T]\n",
        "            \"portfolio_denorm\": portfolio_denorm.float(),\n",
        "            \"S_T_denorm\": S_T_denorm.float(),\n",
        "            \"cashflow_denorm\": cashflow_denorm.float(),\n",
        "            \"derivative_denorm\": derivative_denorm.float()\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo-jBsLXyz4w"
      },
      "source": [
        "#Test with sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrjxgiGvy42A",
        "outputId": "919b6a0c-2576-4ade-8616-67cf01c4457a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Sample 0 ===\n",
            "Portfolio features:\n",
            " tensor([[ 1.5083e+03,  0.0000e+00,  2.5000e-01],\n",
            "        [ 1.4125e+03,  1.0000e+00, -1.0000e+00],\n",
            "        [ 1.3959e+03,  1.0000e+00, -7.5000e-01]])\n",
            "Mask: tensor([True, True, True])\n",
            "S_T: tensor([1812.2474, 1289.1338])\n",
            "Cashflow: tensor([-711.9773,   54.7958])\n",
            "Derivative: tensor([-1.7500, -0.2500])\n",
            "Portfolio (denorm):\n",
            " tensor([[ 1.5083e+03,  0.0000e+00,  2.5000e-01],\n",
            "        [ 1.4125e+03,  1.0000e+00, -1.0000e+00],\n",
            "        [ 1.3959e+03,  1.0000e+00, -7.5000e-01]])\n",
            "S_T (denorm): tensor([1812.2474, 1289.1338])\n",
            "Cashflow (denorm): tensor([-711.9773,   54.7958])\n",
            "Derivative (denorm): tensor([-1.7500, -0.2500])\n",
            "\n",
            "=== Sample 1 ===\n",
            "Portfolio features:\n",
            " tensor([[ 1.1459e+03,  0.0000e+00,  5.0000e-01],\n",
            "        [ 1.4422e+03,  0.0000e+00, -1.0000e+00],\n",
            "        [ 1.3374e+03,  1.0000e+00, -5.0000e-01]])\n",
            "Mask: tensor([True, True, True])\n",
            "S_T: tensor([1472.9598, 1368.7098])\n",
            "Cashflow: tensor([-67.7839, -89.1250])\n",
            "Derivative: tensor([-0.5000,  0.5000])\n",
            "Portfolio (denorm):\n",
            " tensor([[ 1.1459e+03,  0.0000e+00,  5.0000e-01],\n",
            "        [ 1.4422e+03,  0.0000e+00, -1.0000e+00],\n",
            "        [ 1.3374e+03,  1.0000e+00, -5.0000e-01]])\n",
            "S_T (denorm): tensor([1472.9598, 1368.7098])\n",
            "Cashflow (denorm): tensor([-67.7839, -89.1250])\n",
            "Derivative (denorm): tensor([-0.5000,  0.5000])\n"
          ]
        }
      ],
      "source": [
        "# --- Quick test run ---\n",
        "dataset = OperatorDatasetStandardized(\n",
        "    num_samples=2,\n",
        "    num_samples_S_T=2,\n",
        "    K_scaler=None, S_T_scaler=None, cashflow_scaler=None,\n",
        "    max_portfolio_size=3,\n",
        "    min_portfolio_size=1,\n",
        "    is_fitting_mode=True\n",
        ")\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    sample = dataset[i]  # dictionary returned\n",
        "    print(f\"\\n=== Sample {i} ===\")\n",
        "    print(\"Portfolio features:\\n\", sample[\"portfolio\"])\n",
        "    print(\"Mask:\", sample[\"mask\"])\n",
        "    print(\"S_T:\", sample[\"S_T\"])\n",
        "    print(\"Cashflow:\", sample[\"cashflow\"])\n",
        "    print(\"Derivative:\", sample[\"derivative\"])\n",
        "    print(\"Portfolio (denorm):\\n\", sample[\"portfolio_denorm\"])\n",
        "    print(\"S_T (denorm):\", sample[\"S_T_denorm\"])\n",
        "    print(\"Cashflow (denorm):\", sample[\"cashflow_denorm\"])\n",
        "    print(\"Derivative (denorm):\", sample[\"derivative_denorm\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkb8813brKXf",
        "outputId": "306b94e7-1b29-4e89-b6da-cf19e7e57fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cashflow and derivative tests passed!\n",
            "✅ Dataset cashflow and derivative tests passed!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pytest\n",
        "\n",
        "# Dummy constants for testing\n",
        "MU = 0.05\n",
        "T = 1.0\n",
        "NOISE_STD = 0.0  # deterministic\n",
        "\n",
        "# -------------------------\n",
        "# Minimal compute_cashflow\n",
        "# -------------------------\n",
        "def compute_cashflow(portfolio_batch, S_T_batch):\n",
        "    \"\"\"\n",
        "    portfolio_batch: [B, N, 3] tensor\n",
        "    S_T_batch: [B, 1] tensor\n",
        "    Returns: cashflow [B], derivative [B]\n",
        "    \"\"\"\n",
        "    B, N, _ = portfolio_batch.shape\n",
        "    cashflow = torch.zeros(B)\n",
        "    derivative = torch.zeros(B)\n",
        "\n",
        "    for i in range(B):\n",
        "        total = 0.0\n",
        "        delta_total = 0.0\n",
        "        for j in range(N):\n",
        "            K, t, w = portfolio_batch[i, j]\n",
        "            s = S_T_batch[i, 0]\n",
        "            if t == 1:  # call\n",
        "                payoff = max(s - K, 0)\n",
        "                delta = 1.0 if s > K else 0.0\n",
        "            else:       # put\n",
        "                payoff = max(K - s, 0)\n",
        "                delta = -1.0 if s > K else 0.0\n",
        "            total += w * payoff\n",
        "            delta_total += w * delta\n",
        "        cashflow[i] = total\n",
        "        derivative[i] = delta_total\n",
        "    return cashflow, derivative\n",
        "\n",
        "# -------------------------\n",
        "# Manual reference for test\n",
        "# -------------------------\n",
        "def manual_cashflow_delta(S_T, portfolio):\n",
        "    cashflow = []\n",
        "    derivative = []\n",
        "    for s in S_T:\n",
        "        total = 0.0\n",
        "        delta_total = 0.0\n",
        "        for k, t, w in portfolio:\n",
        "            if t == 1:  # call\n",
        "                payoff = max(s - k, 0)\n",
        "                delta = 1.0 if s > k else 0.0\n",
        "            else:       # put\n",
        "                payoff = max(k - s, 0)\n",
        "                delta = -1.0 if s > k else 0.0\n",
        "            total += w * payoff\n",
        "            delta_total += w * delta\n",
        "        cashflow.append(total)\n",
        "        derivative.append(delta_total)\n",
        "    return torch.tensor(cashflow, dtype=torch.float32), torch.tensor(derivative, dtype=torch.float32)\n",
        "\n",
        "# -------------------------\n",
        "# Test cashflow/derivative for known portfolio\n",
        "# -------------------------\n",
        "def test_cashflow_derivative_manual():\n",
        "    portfolio = torch.tensor([[100.0, 1.0, 1.0], [120.0, 0.0, -0.5]], dtype=torch.float32)\n",
        "    S_T = torch.tensor([90.0, 110.0, 130.0], dtype=torch.float32)\n",
        "\n",
        "    portfolio_batch = portfolio.unsqueeze(0).expand(len(S_T), -1, -1)\n",
        "    cashflow, derivative = compute_cashflow(portfolio_batch, S_T.unsqueeze(-1))\n",
        "    expected_cashflow, expected_derivative = manual_cashflow_delta(S_T, portfolio)\n",
        "\n",
        "    assert torch.allclose(cashflow.squeeze(), expected_cashflow, atol=1e-4)\n",
        "    assert torch.allclose(derivative.squeeze(), expected_derivative, atol=1e-4)\n",
        "    print(\"✅ Cashflow and derivative tests passed!\")\n",
        "\n",
        "# -------------------------\n",
        "# Test dataset integration\n",
        "# -------------------------\n",
        "class DummyDataset:\n",
        "    def __getitem__(self, idx):\n",
        "        portfolio = torch.tensor([[100.0, 1.0, 1.0], [120.0, 0.0, -0.5]], dtype=torch.float32)\n",
        "        S_T = torch.tensor([90.0, 110.0, 130.0], dtype=torch.float32)\n",
        "        portfolio_batch = portfolio.unsqueeze(0).expand(len(S_T), -1, -1)\n",
        "        cashflow, derivative = compute_cashflow(portfolio_batch, S_T.unsqueeze(-1))\n",
        "        mask = torch.tensor([True, True], dtype=torch.bool)\n",
        "        # Return same signature as OperatorDatasetStandardized\n",
        "        return {\n",
        "            \"portfolio\": portfolio,\n",
        "            \"mask\": mask,\n",
        "            \"S_T\": S_T,\n",
        "            \"cashflow\": cashflow.squeeze(),\n",
        "            \"derivative\": derivative.squeeze(),\n",
        "            \"portfolio_denorm\": portfolio,\n",
        "            \"S_T_denorm\": S_T,\n",
        "            \"cashflow_denorm\": cashflow.squeeze(),\n",
        "            \"derivative_denorm\": derivative.squeeze()\n",
        "        }\n",
        "\n",
        "def test_dataset_cashflow_derivative():\n",
        "    dataset = DummyDataset()\n",
        "    sample = dataset[0]\n",
        "    expected_cashflow, expected_derivative = manual_cashflow_delta(sample[\"S_T\"], sample[\"portfolio\"])\n",
        "    assert torch.allclose(sample[\"cashflow\"], expected_cashflow, atol=1e-4)\n",
        "    assert torch.allclose(sample[\"derivative\"], expected_derivative, atol=1e-4)\n",
        "    print(\"✅ Dataset cashflow and derivative tests passed!\")\n",
        "\n",
        "# Run tests\n",
        "test_cashflow_derivative_manual()\n",
        "test_dataset_cashflow_derivative()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N0J4G7c8_2-i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/Ucl/\"\n",
        "K_SCALAR_FILE = os.path.join(DRIVE_PATH, 'K_Scalar_Advanced.pkl')\n",
        "ST_SCALAR_FILE = os.path.join(DRIVE_PATH, 'S_T_Scalar_Advanced.pkl')\n",
        "CASHFLOW_SCALAR_FILE = os.path.join(DRIVE_PATH, 'Cashflow_Scalar_Advanced.pkl')\n",
        "\n",
        "\n",
        "def fit_K_ST_scalers(train_loader, save_path_K=K_SCALAR_FILE, save_path_ST=ST_SCALAR_FILE):\n",
        "    print(\"Fitting K and S_T scalers from training set...\")\n",
        "    all_K = []\n",
        "    all_S_T = []\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Collecting K and S_T for scalers\"):\n",
        "        # Access dict keys safely\n",
        "        portfolio_real = batch[\"portfolio\"]  # [B, N, 3]\n",
        "        s_t_real = batch[\"S_T\"]              # [B, num_S_T]\n",
        "\n",
        "        # K is the first column of the portfolio features\n",
        "        K_real = portfolio_real[:, :, 0].cpu().numpy().reshape(-1, 1)\n",
        "        S_T_real = s_t_real.cpu().numpy().reshape(-1, 1)\n",
        "\n",
        "        all_K.append(K_real)\n",
        "        all_S_T.append(S_T_real)\n",
        "\n",
        "    K_all_np = np.concatenate(all_K, axis=0)\n",
        "    S_T_all_np = np.concatenate(all_S_T, axis=0)\n",
        "\n",
        "    K_scalar = StandardScaler()\n",
        "    K_scalar.fit(K_all_np)\n",
        "    #joblib.dump(K_scalar, save_path_K)\n",
        "\n",
        "    S_T_scalar = StandardScaler()\n",
        "    S_T_scalar.fit(S_T_all_np)\n",
        "    #joblib.dump(S_T_scalar, save_path_ST)\n",
        "\n",
        "    # print(f\"✅ Saved K scalar to: {save_path_K}\")\n",
        "    # print(f\"✅ Saved S_T scalar to: {save_path_ST}\")\n",
        "    # print(f\"K mean: {K_scalar.mean_[0]:.4f}, std: {K_scalar.scale_[0]:.4f}\")\n",
        "    # print(f\"S_T mean: {S_T_scalar.mean_[0]:.4f}, std: {S_T_scalar.scale_[0]:.4f}\")\n",
        "\n",
        "    return K_scalar, S_T_scalar\n",
        "\n",
        "\n",
        "def fit_cashflow_scaler(train_loader, save_path=CASHFLOW_SCALAR_FILE):\n",
        "    all_cashflows = []\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Fitting Cashflow Scaler\"):\n",
        "        cashflow = batch[\"cashflow\"]  # [B, N] or [B*num_S_T]\n",
        "        cashflow_np = cashflow.detach().cpu().numpy().reshape(-1, 1)\n",
        "        all_cashflows.append(cashflow_np)\n",
        "\n",
        "    cashflows_np = np.concatenate(all_cashflows, axis=0)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(cashflows_np)\n",
        "    #joblib.dump(scaler, save_path)\n",
        "\n",
        "    # print(f\"✅ Saved Cashflow Scaler to: {save_path}\")\n",
        "    # print(f\"Cashflow Mean: {scaler.mean_[0]:.4f}, Std Dev: {scaler.scale_[0]:.4f}\")\n",
        "\n",
        "    return scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yEf2UaOm_-0S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "FEED_FWD_DEPTH = 3\n",
        "\n",
        "\n",
        "DRPO_OUT_PROB = 0.1\n",
        "\n",
        "\n",
        "class TrunkNet(nn.Module):\n",
        "    def __init__(self, input_dim=1, latent_dim=64, hidden_dim=128,\n",
        "                 num_layers=6, dropout_prob=DRPO_OUT_PROB):\n",
        "        super(TrunkNet, self).__init__()\n",
        "\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_prob)\n",
        "        )\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout_prob)\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_proj = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, S_T):\n",
        "        if S_T.dim() == 1:\n",
        "            S_T = S_T.unsqueeze(-1)\n",
        "        elif S_T.dim() == 2:\n",
        "            S_T = S_T.unsqueeze(-1)\n",
        "\n",
        "        x = self.input_proj(S_T)\n",
        "        for block in self.blocks:\n",
        "            x = x + block(x)\n",
        "        return self.output_proj(x)\n",
        "\n",
        "\n",
        "class ISAB(nn.Module):\n",
        "    \"\"\"Induced Set Attention Block\"\"\"\n",
        "    def __init__(self, d_model, num_heads, num_inds, dropout=DRPO_OUT_PROB):\n",
        "        super().__init__()\n",
        "        self.num_inds = num_inds\n",
        "        self.inducing_points = nn.Parameter(torch.randn(num_inds, d_model))\n",
        "\n",
        "        self.attention1 = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.attention2 = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.norm4 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ffn1 = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * FEED_FWD_DEPTH),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * FEED_FWD_DEPTH, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.ffn2 = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * FEED_FWD_DEPTH),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * FEED_FWD_DEPTH, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size = x.size(0)\n",
        "        I = self.inducing_points.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "\n",
        "        # I attends to X\n",
        "        attn_out1, _ = self.attention1(\n",
        "            I, x, x,\n",
        "            key_padding_mask=(~mask.bool()) if mask is not None else None\n",
        "        )\n",
        "        I = self.norm1(I + attn_out1)\n",
        "        I = self.norm2(I + self.ffn1(I))\n",
        "\n",
        "        # X attends to I\n",
        "        attn_out2, _ = self.attention2(x, I, I)\n",
        "        x = self.norm3(x + attn_out2)\n",
        "        x = self.norm4(x + self.ffn2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PMA(nn.Module):\n",
        "    \"\"\"Pooling by Multihead Attention\"\"\"\n",
        "    def __init__(self, d_model, num_heads, num_seeds, dropout=DRPO_OUT_PROB):\n",
        "        super().__init__()\n",
        "        self.num_seeds = num_seeds\n",
        "        self.seed_vectors = nn.Parameter(torch.randn(num_seeds, d_model))\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * FEED_FWD_DEPTH),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * FEED_FWD_DEPTH, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size = x.size(0)\n",
        "        S = self.seed_vectors.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "\n",
        "        key_padding_mask = (~mask.bool()) if mask is not None else None\n",
        "\n",
        "        attn_out, _ = self.attention(S, x, x, key_padding_mask=key_padding_mask)\n",
        "        S = self.norm1(S + attn_out)\n",
        "        S = self.norm2(S + self.ffn(S))\n",
        "        return S\n",
        "\n",
        "\n",
        "class SAB(nn.Module):\n",
        "    \"\"\"Set Attention Block\"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=DRPO_OUT_PROB):\n",
        "        super().__init__()\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=d_model * FEED_FWD_DEPTH,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        if mask is not None:\n",
        "            src_key_padding_mask = ~mask.bool()\n",
        "        else:\n",
        "            src_key_padding_mask = None\n",
        "        return self.transformer_layer(x, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "\n",
        "class EnhancedSetTransformerEncoder(nn.Module):\n",
        "    \"\"\"Set Transformer with ISAB + PMA\"\"\"\n",
        "    def __init__(self, portfolio_feature_dim=3, latent_dim=128, hidden_dim=64,\n",
        "                 num_heads=2, dropout_prob=DRPO_OUT_PROB, num_inds=32, num_seeds=1,\n",
        "                 use_isab=True, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        if hidden_dim % num_heads != 0:\n",
        "            hidden_dim = ((hidden_dim // num_heads) + 1) * num_heads\n",
        "\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(portfolio_feature_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_prob)\n",
        "        )\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            if use_isab:\n",
        "                self.encoder_layers.append(ISAB(hidden_dim, num_heads, num_inds, dropout_prob))\n",
        "            else:\n",
        "                self.encoder_layers.append(SAB(hidden_dim, num_heads, dropout_prob))\n",
        "\n",
        "        self.pooling = PMA(hidden_dim, num_heads, num_seeds, dropout_prob)\n",
        "\n",
        "        self.output_proj = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, portfolio, mask=None):\n",
        "        x = self.input_proj(portfolio)  # [B, P, H]\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, mask=mask)\n",
        "        x = self.pooling(x, mask=mask)\n",
        "        x = x.mean(dim=1) if x.size(1) > 1 else x.squeeze(1)\n",
        "        return self.output_proj(x)\n",
        "\n",
        "\n",
        "class OptimizedSetTransformerEncoder(nn.Module):\n",
        "    \"\"\"Vanilla Set Transformer with mask support\"\"\"\n",
        "    def __init__(self, portfolio_feature_dim=3, latent_dim=128, hidden_dim=64,\n",
        "                 num_layers=1, num_heads=2, dropout_prob=DRPO_OUT_PROB):\n",
        "        super().__init__()\n",
        "\n",
        "        if hidden_dim % num_heads != 0:\n",
        "            hidden_dim = ((hidden_dim // num_heads) + 1) * num_heads\n",
        "\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(portfolio_feature_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_prob)\n",
        "        )\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * FEED_FWD_DEPTH,\n",
        "            dropout=dropout_prob,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "            enable_nested_tensor=False\n",
        "        )\n",
        "\n",
        "        self.output_proj = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, portfolio, mask=None):\n",
        "        x = self.input_proj(portfolio)\n",
        "        src_key_padding_mask = ~mask.bool() if mask is not None else None\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
        "        pooled = x.mean(dim=1) + x.max(dim=1).values\n",
        "        return self.output_proj(pooled)\n",
        "\n",
        "\n",
        "class OptimizedDeepONet(nn.Module):\n",
        "    \"\"\"DeepONet with branch (portfolio) + trunk (S_T)\"\"\"\n",
        "    def __init__(self, portfolio_feature_dim=3, hidden_dim=64, latent_dim=128,\n",
        "                 dropout_prob=DRPO_OUT_PROB, num_heads=2, use_enhanced_transformer=True,\n",
        "                 num_inds=32, num_seeds=1):\n",
        "        super().__init__()\n",
        "\n",
        "        if hidden_dim % num_heads != 0:\n",
        "            recommended = ((hidden_dim // num_heads) + 1) * num_heads\n",
        "            raise ValueError(\n",
        "                f\"hidden_dim ({hidden_dim}) must be divisible by num_heads ({num_heads}). \"\n",
        "                f\"Try hidden_dim={recommended}\"\n",
        "            )\n",
        "\n",
        "        if use_enhanced_transformer:\n",
        "            self.branch_net = EnhancedSetTransformerEncoder(\n",
        "                portfolio_feature_dim=portfolio_feature_dim,\n",
        "                latent_dim=latent_dim,\n",
        "                hidden_dim=hidden_dim,\n",
        "                num_heads=num_heads,\n",
        "                dropout_prob=dropout_prob,\n",
        "                num_inds=num_inds,\n",
        "                num_seeds=num_seeds,\n",
        "                use_isab=True,\n",
        "                num_layers=2\n",
        "            )\n",
        "        else:\n",
        "            self.branch_net = OptimizedSetTransformerEncoder(\n",
        "                portfolio_feature_dim=portfolio_feature_dim,\n",
        "                latent_dim=latent_dim,\n",
        "                hidden_dim=hidden_dim,\n",
        "                dropout_prob=dropout_prob,\n",
        "                num_heads=num_heads\n",
        "            )\n",
        "\n",
        "        self.trunk_net = TrunkNet(\n",
        "            input_dim=1,\n",
        "            latent_dim=latent_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            dropout_prob=dropout_prob\n",
        "        )\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "        self.branch_scale = nn.Parameter(torch.ones(1) * 0.8)\n",
        "        self.trunk_scale = nn.Parameter(torch.ones(1) * 0.8)\n",
        "\n",
        "    def forward(self, portfolio, S_T, mask=None):\n",
        "        B, M = S_T.shape\n",
        "\n",
        "        branch_out = self.branch_net(portfolio, mask=mask) * self.branch_scale\n",
        "\n",
        "        trunk_outputs = []\n",
        "        for i in range(M):\n",
        "            S_T_single = S_T[:, i:i+1]\n",
        "            trunk_out = self.trunk_net(S_T_single) * self.trunk_scale\n",
        "            trunk_outputs.append(trunk_out)\n",
        "\n",
        "        trunk_out = torch.cat(trunk_outputs, dim=1)  # [B, M, latent_dim]\n",
        "\n",
        "        branch_expanded = branch_out.unsqueeze(1)  # [B, 1, latent_dim]\n",
        "        interaction = (branch_expanded * trunk_out).sum(dim=-1)  # [B, M]\n",
        "\n",
        "        return interaction + self.bias\n",
        "\n",
        "\n",
        "        # branch_out = self.branch_net(portfolio, mask=mask) * self.branch_scale\n",
        "        # trunk_out = self.trunk_net(S_T) * self.trunk_scale\n",
        "        # branch_expanded = branch_out.unsqueeze(1)\n",
        "        # interaction = (branch_expanded * trunk_out).sum(dim=-1)\n",
        "        # return interaction + self.bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vR_SMxGZAG6a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.amp import GradScaler\n",
        "\n",
        "# --- Gradient summary helper ---\n",
        "def compute_gradient_stats(model):\n",
        "    gradient_stats = {}\n",
        "    total_norm = 0.0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_norm = param.grad.norm().item()\n",
        "            total_norm += grad_norm ** 2\n",
        "            gradient_stats[name] = {\n",
        "                'norm': grad_norm,\n",
        "                'shape': tuple(param.grad.shape),\n",
        "                'numel': param.grad.numel(),\n",
        "                'mean': param.grad.mean().item(),\n",
        "                'std': param.grad.std().item()\n",
        "            }\n",
        "    total_norm = total_norm ** 0.5\n",
        "    return total_norm, gradient_stats, None\n",
        "\n",
        "def print_gradient_summary(gradient_stats, total_norm, epoch, batch_idx=None):\n",
        "    prefix = f\"Epoch {epoch}\" + (f\", Batch {batch_idx}\" if batch_idx is not None else \"\")\n",
        "    print(f\"\\n🔍 === Gradient Analysis - {prefix} ===\")\n",
        "    print(f\"Total Gradient Norm: {total_norm:.6f}\")\n",
        "\n",
        "    if total_norm > 30.0:\n",
        "        print(\"🚨 CRITICAL: Severe gradient explosion! Consider stopping training.\")\n",
        "    elif total_norm > 20.0:\n",
        "        print(\"⚠️  SEVERE: Major gradient explosion detected!\")\n",
        "    elif total_norm > 10.0:\n",
        "        print(\"⚠️  WARNING: Moderate gradient explosion detected!\")\n",
        "    elif total_norm < 1e-6:\n",
        "        print(\"⚠️  WARNING: Vanishing gradients detected!\")\n",
        "    else:\n",
        "        print(\"✅ Gradient norm is healthy\")\n",
        "\n",
        "    sorted_layers = sorted(gradient_stats.items(), key=lambda x: x[1]['norm'], reverse=True)\n",
        "    print(f\"\\nTop 5 layers by gradient norm (out of {len(gradient_stats)} total):\")\n",
        "    for i, (layer_name, stats) in enumerate(sorted_layers[:5]):\n",
        "        status = \"🔥\" if stats['norm'] > 3.0 else \"⚠️\" if stats['norm'] > 1.0 else \"✅\"\n",
        "        print(f\"  {status} {i+1}. {layer_name}: {stats['norm']:.4f}\")\n",
        "        print(f\"      Shape: {stats['shape']}, Elements: {stats['numel']}\")\n",
        "        print(f\"      Mean: {stats['mean']:.6f}, Std: {stats['std']:.6f}\")\n",
        "    print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tZQV7h_KAKlj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "class ExtendedEarlyStopping:\n",
        "    # ... (no changes needed here) ...\n",
        "    def __init__(self, patience=30, min_delta=0.0005, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.best = float('inf')\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model=None):\n",
        "        if val_loss < self.best - self.min_delta:\n",
        "            self.best = val_loss\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "\n",
        "        if self.wait >= self.patience:\n",
        "            self.stopped_epoch = True\n",
        "            if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "\n",
        "        return self.stopped_epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "F8vpybx_AM8N"
      },
      "outputs": [],
      "source": [
        "def get_stable_hyperparameters():\n",
        "    \"\"\"Return more stable hyperparameters\"\"\"\n",
        "    return {\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"weight_decay\": 1e-4 ,\n",
        "        \"lambda_deriv\": 0.05,\n",
        "        \"lambda_reg\": 1e-4,\n",
        "        \"gradient_clip_norm\": 5,\n",
        "        \"batch_size\": 128,\n",
        "        \"scheduler_T0\": 5,\n",
        "        \"early_stopping_patience\": 35,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "2Jx4Xu-nAPH9"
      },
      "outputs": [],
      "source": [
        "\n",
        "class OptimizedTrainer:\n",
        "    def __init__(self, model, device='cuda', monitor_gradients=True,\n",
        "                 learning_rate=5e-6, lambda_deriv_weight=0.1, weight_decay=1e-4,\n",
        "                 scale_warmup_epochs=5, initial_scale=0.05, final_scale=1.0, grad_log_threshold = 5.0):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.monitor_gradients = monitor_gradients\n",
        "        self.lambda_deriv_weight = lambda_deriv_weight\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8\n",
        "        )\n",
        "\n",
        "        # Cosine Annealing Warm Restarts scheduler (epoch-based)\n",
        "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            self.optimizer, T_0=scale_warmup_epochs, T_mult=1, eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        # Gradient scaler for mixed precision\n",
        "        self.scaler = GradScaler()\n",
        "        self.huber_loss = nn.SmoothL1Loss(beta=1.0)\n",
        "\n",
        "        # Branch/trunk scale\n",
        "        self.scale_warmup_epochs = scale_warmup_epochs\n",
        "        self.initial_scale = initial_scale\n",
        "        self.final_scale = final_scale\n",
        "        if hasattr(self.model, 'branch_scale') and hasattr(self.model, 'trunk_scale'):\n",
        "            with torch.no_grad():\n",
        "                self.model.branch_scale.fill_(initial_scale)\n",
        "                self.model.trunk_scale.fill_(initial_scale)\n",
        "\n",
        "    def check_model_health(self, epoch, batch_idx):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if torch.isnan(param).any() or torch.isinf(param).any():\n",
        "                print(f\"Bad parameter: {name} at Epoch {epoch}, Batch {batch_idx}\")\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def compute_loss(self, pred_cashflow, true_cashflow, pred_deriv=None, true_deriv=None, mask=None):\n",
        "        \"\"\"\n",
        "        FIXED: Computes total loss correctly without misapplying masks.\n",
        "\n",
        "        Key insight: Portfolio mask is for MODEL INPUTS (which options are valid),\n",
        "        not for LOSS COMPUTATION (all scenarios are always valid).\n",
        "\n",
        "        Args:\n",
        "            pred_cashflow: [B, M] predicted cashflows for M scenarios\n",
        "            true_cashflow: [B, M] true cashflows for M scenarios\n",
        "            pred_deriv: [B, M] predicted derivatives for M scenarios\n",
        "            true_deriv: [B, M] true derivatives for M scenarios\n",
        "            mask: [B, N] portfolio mask (NOT used in loss computation)\n",
        "        \"\"\"\n",
        "\n",
        "        # --- Cashflow loss (no masking needed) ---\n",
        "        # All scenarios are always valid, regardless of portfolio structure\n",
        "        cashflow_loss = self.huber_loss(pred_cashflow, true_cashflow)\n",
        "\n",
        "        # --- Derivative loss (no masking needed) ---\n",
        "        if pred_deriv is not None and true_deriv is not None:\n",
        "            deriv_loss = self.huber_loss(pred_deriv, true_deriv)\n",
        "            total_loss = cashflow_loss + self.lambda_deriv_weight * deriv_loss\n",
        "        else:\n",
        "            deriv_loss = torch.tensor(0.0, device=pred_cashflow.device)\n",
        "            total_loss = cashflow_loss\n",
        "\n",
        "        return total_loss, cashflow_loss, deriv_loss\n",
        "\n",
        "    def train_step(self, portfolio, S_T, cashflow, true_derivative=None, mask=None, epoch=0, batch_idx=0, log_gradients=False):\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      S_T = S_T.clone().detach().requires_grad_(True).to(self.device)\n",
        "      portfolio = portfolio.to(self.device)\n",
        "      cashflow = cashflow.to(self.device)\n",
        "      if mask is not None:\n",
        "          mask = mask.to(self.device)\n",
        "      if true_derivative is not None:\n",
        "          true_derivative = true_derivative.to(self.device)\n",
        "\n",
        "      pred_cashflow = self.model(portfolio, S_T, mask=mask)\n",
        "\n",
        "      pred_deriv = None\n",
        "      if true_derivative is not None:\n",
        "          # FIXED: Compute gradients per scenario, not summed\n",
        "          pred_deriv = torch.autograd.grad(\n",
        "              outputs=pred_cashflow,\n",
        "              inputs=S_T,\n",
        "              grad_outputs=torch.ones_like(pred_cashflow),  # ADD THIS LINE\n",
        "              retain_graph=True,\n",
        "              create_graph=True,\n",
        "              allow_unused=True\n",
        "          )[0]\n",
        "\n",
        "      total_loss, cashflow_loss, deriv_loss = self.compute_loss(\n",
        "          pred_cashflow, cashflow, pred_deriv, true_derivative, mask=None\n",
        "      )\n",
        "\n",
        "      self.scaler.scale(total_loss).backward()\n",
        "      self.scaler.unscale_(self.optimizer)\n",
        "      torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
        "      self.scaler.step(self.optimizer)\n",
        "      self.scaler.update()\n",
        "\n",
        "      return total_loss.item(), cashflow_loss.item(), deriv_loss.item()\n",
        "\n",
        "    def val_step(self, portfolio, S_T, cashflow, true_derivative=None, mask=None):\n",
        "      \"\"\"\n",
        "      Validation step that can compute derivative loss as well.\n",
        "      \"\"\"\n",
        "      self.model.eval()\n",
        "\n",
        "      portfolio = portfolio.to(self.device)\n",
        "      S_T = S_T.clone().detach().requires_grad_(True).to(self.device)  # need grad for derivative\n",
        "      cashflow = cashflow.to(self.device)\n",
        "      if mask is not None:\n",
        "          mask = mask.to(self.device)\n",
        "      if true_derivative is not None:\n",
        "          true_derivative = true_derivative.to(self.device)\n",
        "\n",
        "      # always compute forward with grad enabled (so we can use for both losses)\n",
        "      with torch.enable_grad():\n",
        "          pred_cashflow = self.model(portfolio, S_T, mask=mask)\n",
        "\n",
        "          pred_deriv = None\n",
        "          if true_derivative is not None:\n",
        "              pred_deriv = torch.autograd.grad(\n",
        "                  outputs=pred_cashflow,\n",
        "                  inputs=S_T,\n",
        "                  grad_outputs=torch.ones_like(pred_cashflow),\n",
        "                  retain_graph=False,\n",
        "                  create_graph=False,\n",
        "                  allow_unused=True\n",
        "              )[0]\n",
        "\n",
        "      # detach before computing loss to avoid holding graph in memory\n",
        "      total_loss, cashflow_loss, deriv_loss = self.compute_loss(\n",
        "          pred_cashflow.detach(), cashflow,\n",
        "          pred_deriv.detach() if pred_deriv is not None else None,\n",
        "          true_derivative, mask=None\n",
        "      )\n",
        "\n",
        "      return total_loss.item(), cashflow_loss.item(), deriv_loss.item()\n",
        "\n",
        "\n",
        "    def update_scale(self, current_epoch):\n",
        "        \"\"\"Update branch/trunk scale during warmup.\"\"\"\n",
        "        if hasattr(self.model, 'branch_scale') and hasattr(self.model, 'trunk_scale'):\n",
        "            factor = min((current_epoch + 1) / self.scale_warmup_epochs, 1.0)\n",
        "            new_scale = self.initial_scale + (self.final_scale - self.initial_scale) * factor\n",
        "            with torch.no_grad():\n",
        "                self.model.branch_scale.fill_(new_scale)\n",
        "                self.model.trunk_scale.fill_(new_scale)\n",
        "            print(f\"[Epoch {current_epoch}] Updated branch/trunk scale → {new_scale:.4f}\")\n",
        "\n",
        "    def step_scheduler_epoch(self):\n",
        "        \"\"\"Step scheduler once per epoch.\"\"\"\n",
        "        self.scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWSulG29A67D",
        "outputId": "b4008c04-acfd-453e-b491-bd55f28a130e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : interesting_taper_4184\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/satyabratkumarsingh/option-portfolio-encoder-decoder/10838a5cf05d46fab30167e6bb067314\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_cashflow_loss_batch [1120]   : (0.10014146566390991, 0.71085125207901)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_cashflow_loss_epoch [7]      : (0.1341879060026258, 0.2814498905092478)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_derivative_loss_batch [1120] : (0.3015000820159912, 1.0325230360031128)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_derivative_loss_epoch [7]    : (0.45259464904665947, 0.6696506692096591)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_total_loss_batch [1120]      : (0.11924313008785248, 0.7454009652137756)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_total_loss_epoch [7]         : (0.15681763822212816, 0.3149324242956936)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_cashflow_loss_batch [256]      : (0.0835786983370781, 0.26101264357566833)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_cashflow_loss_epoch [6]        : (0.10542835984379054, 0.22959099039435388)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_derivative_loss_batch [256]    : (0.3240087926387787, 0.7881559729576111)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_derivative_loss_epoch [6]      : (0.44775512516498567, 0.6007967226207256)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_total_loss_batch [256]         : (0.10208197683095932, 0.2949678897857666)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_total_loss_epoch [6]           : (0.12781611662358044, 0.2596308272331953)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook_url : https://colab.research.google.com/notebook#fileId=1TLyI2xBR6xkR27v_-k4KOh5pR5G3xcWp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size              : 128\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     early_stopping_patience : 30\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gradient_clip_norm      : 5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lambda_deriv            : 0.05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lambda_reg              : 0.0001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate           : 0.0003\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scheduler_T0            : 5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay            : 5e-05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n"
          ]
        }
      ],
      "source": [
        "experiment.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q3qaeMO54UN"
      },
      "source": [
        "# Model save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "Z-613yyP5-Od"
      },
      "outputs": [],
      "source": [
        "def save_model_checkpoint(model, save_path, epoch=None, optimizer=None, scheduler=None,\n",
        "                         train_loss=None, val_loss=None, train_size=None, val_size=None):\n",
        "\n",
        "    checkpoint_data = {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"hparams\": {\n",
        "            \"hidden_dim\": hidden_dim,\n",
        "            \"latent_dim\": latent_dim,\n",
        "            \"portfolio_feature_dim\": portfolio_feature_dim,\n",
        "            \"use_enhanced_transformer\": True\n",
        "        },\n",
        "        \"training_config\": {\n",
        "            \"PORT_LEN\": PORT_LEN,\n",
        "            \"PORT_SAMPLE_SIZE\": PORT_SAMPLE_SIZE,\n",
        "            \"FEED_ST_LEN_EACH_PORT\": FEED_ST_LEN_EACH_PORT,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"train_size\": train_size,\n",
        "            \"val_size\": val_size\n",
        "        },\n",
        "        \"scaler_files\": {\n",
        "            \"K_scaler\": \"K_Scalar_Training.pkl\",\n",
        "            \"S_T_scaler\": \"S_T_Scalar_Training.pkl\",\n",
        "            \"cashflow_scaler\": \"Cashflow_Scalar_Training.pkl\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Add optional training state information\n",
        "    if epoch is not None:\n",
        "        checkpoint_data[\"epoch\"] = epoch\n",
        "    if optimizer is not None:\n",
        "        checkpoint_data[\"optimizer_state_dict\"] = optimizer.state_dict()\n",
        "    if scheduler is not None:\n",
        "        checkpoint_data[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
        "    if train_loss is not None:\n",
        "        checkpoint_data[\"train_loss\"] = train_loss\n",
        "    if val_loss is not None:\n",
        "        checkpoint_data[\"val_loss\"] = val_loss\n",
        "\n",
        "    torch.save(checkpoint_data, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlhMf78VARuG",
        "outputId": "01719256-d7ba-4c22-80b5-b3ff5dffb9ae"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/satyabratkumarsingh/option-portfolio-encoder-decoder/96ac953a751041d48e6b2089d00b1db8\n",
            "\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting scalers on training data...\n",
            "Fitting K and S_T scalers from training set...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Collecting K and S_T for scalers: 100%|██████████| 160/160 [00:30<00:00,  5.18it/s]\n",
            "Fitting Cashflow Scaler: 100%|██████████| 160/160 [00:30<00:00,  5.20it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving scalers for evaluation consistency...\n",
            "✅ Successfully saved all training scalers\n",
            "📊 Scaler Statistics:\n",
            "K_scaler - mean: 155.0788, std: 175.6431\n",
            "S_T_scaler - mean: 313.4675, std: 138.1498\n",
            "Cashflow_scaler - mean: 0.4324, std: 167.9381\n",
            "[Epoch 0] Updated branch/trunk scale → 0.2400\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 0: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 → Current LR: 2.714480e-04\n",
            "Epoch [0/500] Train → total: 0.333914, cf: 0.298260, deriv: 0.713087 | Val → total: 0.304020, cf: 0.269714, deriv: 0.686130\n",
            "[Epoch 1] Updated branch/trunk scale → 0.4300\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 1: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 → Current LR: 1.966980e-04\n",
            "Epoch [1/500] Train → total: 0.304801, cf: 0.271661, deriv: 0.662811 | Val → total: 0.259948, cf: 0.228533, deriv: 0.628292\n",
            "[Epoch 2] Updated branch/trunk scale → 0.6200\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 → Current LR: 1.043020e-04\n",
            "Epoch [2/500] Train → total: 0.247491, cf: 0.218845, deriv: 0.572919 | Val → total: 0.163104, cf: 0.138047, deriv: 0.501137\n",
            "[Epoch 3] Updated branch/trunk scale → 0.8100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 → Current LR: 2.955196e-05\n",
            "Epoch [3/500] Train → total: 0.224265, cf: 0.198146, deriv: 0.522382 | Val → total: 0.152714, cf: 0.128717, deriv: 0.479952\n",
            "[Epoch 4] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 → Current LR: 3.000000e-04\n",
            "Epoch [4/500] Train → total: 0.229259, cf: 0.203710, deriv: 0.510979 | Val → total: 0.142427, cf: 0.119407, deriv: 0.460408\n",
            "[Epoch 5] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 → Current LR: 2.714480e-04\n",
            "Epoch [5/500] Train → total: 0.244192, cf: 0.218878, deriv: 0.506285 | Val → total: 0.182052, cf: 0.158838, deriv: 0.464284\n",
            "[Epoch 6] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 → Current LR: 1.966980e-04\n",
            "Epoch [6/500] Train → total: 0.185356, cf: 0.160718, deriv: 0.492771 | Val → total: 0.142017, cf: 0.118479, deriv: 0.470764\n",
            "[Epoch 7] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 → Current LR: 1.043020e-04\n",
            "Epoch [7/500] Train → total: 0.160205, cf: 0.137006, deriv: 0.463969 | Val → total: 0.131419, cf: 0.107203, deriv: 0.484313\n",
            "[Epoch 8] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 → Current LR: 2.955196e-05\n",
            "Epoch [8/500] Train → total: 0.141416, cf: 0.119016, deriv: 0.447999 | Val → total: 0.133051, cf: 0.108610, deriv: 0.488823\n",
            "[Epoch 9] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 → Current LR: 3.000000e-04\n",
            "Epoch [9/500] Train → total: 0.132082, cf: 0.110178, deriv: 0.438074 | Val → total: 0.124859, cf: 0.099331, deriv: 0.510561\n",
            "[Epoch 10] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 → Current LR: 2.714480e-04\n",
            "Epoch [10/500] Train → total: 0.152893, cf: 0.130390, deriv: 0.450074 | Val → total: 0.147251, cf: 0.123258, deriv: 0.479853\n",
            "[Epoch 11] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 11: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 → Current LR: 1.966980e-04\n",
            "Epoch [11/500] Train → total: 0.134895, cf: 0.112799, deriv: 0.441911 | Val → total: 0.129448, cf: 0.103712, deriv: 0.514721\n",
            "[Epoch 12] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 12: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 → Current LR: 1.043020e-04\n",
            "Epoch [12/500] Train → total: 0.127223, cf: 0.105173, deriv: 0.440999 | Val → total: 0.121794, cf: 0.096783, deriv: 0.500232\n",
            "[Epoch 13] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 13: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 → Current LR: 2.955196e-05\n",
            "Epoch [13/500] Train → total: 0.110838, cf: 0.090207, deriv: 0.412619 | Val → total: 0.129814, cf: 0.102430, deriv: 0.547694\n",
            "[Epoch 14] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 14: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 → Current LR: 3.000000e-04\n",
            "Epoch [14/500] Train → total: 0.106001, cf: 0.085314, deriv: 0.413731 | Val → total: 0.157279, cf: 0.126618, deriv: 0.613216\n",
            "[Epoch 15] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 15: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 → Current LR: 2.714480e-04\n",
            "Epoch [15/500] Train → total: 0.123812, cf: 0.102406, deriv: 0.428118 | Val → total: 0.156090, cf: 0.126907, deriv: 0.583671\n",
            "[Epoch 16] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 16: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 → Current LR: 1.966980e-04\n",
            "Epoch [16/500] Train → total: 0.116134, cf: 0.095354, deriv: 0.415590 | Val → total: 0.130627, cf: 0.103582, deriv: 0.540905\n",
            "[Epoch 17] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 17: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 → Current LR: 1.043020e-04\n",
            "Epoch [17/500] Train → total: 0.106796, cf: 0.086150, deriv: 0.412929 | Val → total: 0.149345, cf: 0.120405, deriv: 0.578791\n",
            "[Epoch 18] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 18: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 → Current LR: 2.955196e-05\n",
            "Epoch [18/500] Train → total: 0.097641, cf: 0.077375, deriv: 0.405313 | Val → total: 0.136264, cf: 0.105762, deriv: 0.610039\n",
            "[Epoch 19] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 19: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 → Current LR: 3.000000e-04\n",
            "Epoch [19/500] Train → total: 0.092961, cf: 0.072979, deriv: 0.399647 | Val → total: 0.150350, cf: 0.118285, deriv: 0.641298\n",
            "[Epoch 20] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 20: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 → Current LR: 2.714480e-04\n",
            "Epoch [20/500] Train → total: 0.109618, cf: 0.089509, deriv: 0.402193 | Val → total: 0.151122, cf: 0.120222, deriv: 0.618010\n",
            "[Epoch 21] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 21: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 → Current LR: 1.966980e-04\n",
            "Epoch [21/500] Train → total: 0.103538, cf: 0.083355, deriv: 0.403654 | Val → total: 0.155965, cf: 0.123448, deriv: 0.650337\n",
            "[Epoch 22] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 22: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 → Current LR: 1.043020e-04\n",
            "Epoch [22/500] Train → total: 0.096936, cf: 0.076218, deriv: 0.414353 | Val → total: 0.143295, cf: 0.112409, deriv: 0.617738\n",
            "[Epoch 23] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 23: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23 → Current LR: 2.955196e-05\n",
            "Epoch [23/500] Train → total: 0.091583, cf: 0.071235, deriv: 0.406956 | Val → total: 0.130710, cf: 0.100922, deriv: 0.595756\n",
            "[Epoch 24] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 24: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24 → Current LR: 3.000000e-04\n",
            "Epoch [24/500] Train → total: 0.085458, cf: 0.065414, deriv: 0.400884 | Val → total: 0.141071, cf: 0.109770, deriv: 0.626011\n",
            "[Epoch 25] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 25: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25 → Current LR: 2.714480e-04\n",
            "Epoch [25/500] Train → total: 0.101566, cf: 0.081219, deriv: 0.406946 | Val → total: 0.167414, cf: 0.138842, deriv: 0.571440\n",
            "[Epoch 26] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 26: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26 → Current LR: 1.966980e-04\n",
            "Epoch [26/500] Train → total: 0.097294, cf: 0.077180, deriv: 0.402286 | Val → total: 0.119417, cf: 0.093599, deriv: 0.516364\n",
            "[Epoch 27] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 27: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27 → Current LR: 1.043020e-04\n",
            "Epoch [27/500] Train → total: 0.090002, cf: 0.069787, deriv: 0.404318 | Val → total: 0.169629, cf: 0.139018, deriv: 0.612224\n",
            "[Epoch 28] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 28: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28 → Current LR: 2.955196e-05\n",
            "Epoch [28/500] Train → total: 0.083657, cf: 0.063616, deriv: 0.400821 | Val → total: 0.150215, cf: 0.117551, deriv: 0.653290\n",
            "[Epoch 29] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 29: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29 → Current LR: 3.000000e-04\n",
            "Epoch [29/500] Train → total: 0.080182, cf: 0.059966, deriv: 0.404327 | Val → total: 0.137949, cf: 0.106605, deriv: 0.626882\n",
            "[Epoch 30] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 30: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30 → Current LR: 2.714480e-04\n",
            "Epoch [30/500] Train → total: 0.098240, cf: 0.078002, deriv: 0.404765 | Val → total: 0.101819, cf: 0.075969, deriv: 0.517011\n",
            "[Epoch 31] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 31: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31 → Current LR: 1.966980e-04\n",
            "Epoch [31/500] Train → total: 0.092240, cf: 0.071853, deriv: 0.407738 | Val → total: 0.166313, cf: 0.133661, deriv: 0.653038\n",
            "[Epoch 32] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 32: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32 → Current LR: 1.043020e-04\n",
            "Epoch [32/500] Train → total: 0.086798, cf: 0.066588, deriv: 0.404196 | Val → total: 0.117981, cf: 0.092773, deriv: 0.504156\n",
            "[Epoch 33] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 33: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33 → Current LR: 2.955196e-05\n",
            "Epoch [33/500] Train → total: 0.080033, cf: 0.060104, deriv: 0.398591 | Val → total: 0.133120, cf: 0.104209, deriv: 0.578230\n",
            "[Epoch 34] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 34: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34 → Current LR: 3.000000e-04\n",
            "Epoch [34/500] Train → total: 0.076662, cf: 0.056053, deriv: 0.412173 | Val → total: 0.136974, cf: 0.104904, deriv: 0.641405\n",
            "[Epoch 35] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 35: 100%|██████████| 160/160 [03:46<00:00,  1.41s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35 → Current LR: 2.714480e-04\n",
            "Epoch [35/500] Train → total: 0.090215, cf: 0.069871, deriv: 0.406881 | Val → total: 0.109462, cf: 0.082592, deriv: 0.537388\n",
            "[Epoch 36] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 36: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36 → Current LR: 1.966980e-04\n",
            "Epoch [36/500] Train → total: 0.089669, cf: 0.069718, deriv: 0.399026 | Val → total: 0.130433, cf: 0.099468, deriv: 0.619298\n",
            "[Epoch 37] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 37: 100%|██████████| 160/160 [03:46<00:00,  1.41s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37 → Current LR: 1.043020e-04\n",
            "Epoch [37/500] Train → total: 0.082947, cf: 0.062767, deriv: 0.403603 | Val → total: 0.147785, cf: 0.115422, deriv: 0.647272\n",
            "[Epoch 38] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 38: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38 → Current LR: 2.955196e-05\n",
            "Epoch [38/500] Train → total: 0.077603, cf: 0.057446, deriv: 0.403141 | Val → total: 0.145884, cf: 0.113381, deriv: 0.650064\n",
            "[Epoch 39] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 39: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39 → Current LR: 3.000000e-04\n",
            "Epoch [39/500] Train → total: 0.074148, cf: 0.053610, deriv: 0.410774 | Val → total: 0.125796, cf: 0.095406, deriv: 0.607811\n",
            "[Epoch 40] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 40: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40 → Current LR: 2.714480e-04\n",
            "Epoch [40/500] Train → total: 0.087559, cf: 0.067528, deriv: 0.400613 | Val → total: 0.140384, cf: 0.110676, deriv: 0.594151\n",
            "[Epoch 41] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 41: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41 → Current LR: 1.966980e-04\n",
            "Epoch [41/500] Train → total: 0.086470, cf: 0.066215, deriv: 0.405112 | Val → total: 0.115011, cf: 0.085668, deriv: 0.586852\n",
            "[Epoch 42] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 42: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42 → Current LR: 1.043020e-04\n",
            "Epoch [42/500] Train → total: 0.078925, cf: 0.059351, deriv: 0.391490 | Val → total: 0.115548, cf: 0.086215, deriv: 0.586659\n",
            "[Epoch 43] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 43: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43 → Current LR: 2.955196e-05\n",
            "Epoch [43/500] Train → total: 0.074408, cf: 0.054353, deriv: 0.401101 | Val → total: 0.122357, cf: 0.093554, deriv: 0.576055\n",
            "[Epoch 44] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 44: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44 → Current LR: 3.000000e-04\n",
            "Epoch [44/500] Train → total: 0.070591, cf: 0.050662, deriv: 0.398587 | Val → total: 0.123813, cf: 0.093051, deriv: 0.615249\n",
            "[Epoch 45] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 45: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45 → Current LR: 2.714480e-04\n",
            "Epoch [45/500] Train → total: 0.089563, cf: 0.069566, deriv: 0.399937 | Val → total: 0.103638, cf: 0.077040, deriv: 0.531967\n",
            "[Epoch 46] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 46: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46 → Current LR: 1.966980e-04\n",
            "Epoch [46/500] Train → total: 0.079385, cf: 0.059475, deriv: 0.398192 | Val → total: 0.165080, cf: 0.133861, deriv: 0.624371\n",
            "[Epoch 47] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 47: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47 → Current LR: 1.043020e-04\n",
            "Epoch [47/500] Train → total: 0.076327, cf: 0.056309, deriv: 0.400350 | Val → total: 0.124295, cf: 0.095839, deriv: 0.569116\n",
            "[Epoch 48] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 48: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48 → Current LR: 2.955196e-05\n",
            "Epoch [48/500] Train → total: 0.071576, cf: 0.051679, deriv: 0.397934 | Val → total: 0.114877, cf: 0.084663, deriv: 0.604275\n",
            "[Epoch 49] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 49: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49 → Current LR: 3.000000e-04\n",
            "Epoch [49/500] Train → total: 0.068438, cf: 0.048994, deriv: 0.388894 | Val → total: 0.125820, cf: 0.095161, deriv: 0.613188\n",
            "🔄 Model checkpoint saved at epoch 50: /content/drive/MyDrive/Ucl/checkpoints/deeponet_model-epoch50.pt\n",
            "[Epoch 50] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 50: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50 → Current LR: 2.714480e-04\n",
            "Epoch [50/500] Train → total: 0.080226, cf: 0.060640, deriv: 0.391725 | Val → total: 0.132490, cf: 0.101270, deriv: 0.624399\n",
            "[Epoch 51] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 51: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 51 → Current LR: 1.966980e-04\n",
            "Epoch [51/500] Train → total: 0.077986, cf: 0.058233, deriv: 0.395064 | Val → total: 0.085089, cf: 0.060849, deriv: 0.484791\n",
            "[Epoch 52] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 52: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 52 → Current LR: 1.043020e-04\n",
            "Epoch [52/500] Train → total: 0.073379, cf: 0.053887, deriv: 0.389842 | Val → total: 0.119289, cf: 0.089342, deriv: 0.598937\n",
            "[Epoch 53] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 53: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 53 → Current LR: 2.955196e-05\n",
            "Epoch [53/500] Train → total: 0.068744, cf: 0.049681, deriv: 0.381258 | Val → total: 0.125761, cf: 0.094270, deriv: 0.629833\n",
            "[Epoch 54] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 54: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 54 → Current LR: 3.000000e-04\n",
            "Epoch [54/500] Train → total: 0.066751, cf: 0.047679, deriv: 0.381458 | Val → total: 0.122791, cf: 0.091252, deriv: 0.630778\n",
            "[Epoch 55] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 55: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 55 → Current LR: 2.714480e-04\n",
            "Epoch [55/500] Train → total: 0.077289, cf: 0.058184, deriv: 0.382114 | Val → total: 0.101895, cf: 0.075103, deriv: 0.535833\n",
            "[Epoch 56] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 56: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 56 → Current LR: 1.966980e-04\n",
            "Epoch [56/500] Train → total: 0.078675, cf: 0.058721, deriv: 0.399069 | Val → total: 0.119875, cf: 0.091226, deriv: 0.572975\n",
            "[Epoch 57] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 57: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 57 → Current LR: 1.043020e-04\n",
            "Epoch [57/500] Train → total: 0.074769, cf: 0.054664, deriv: 0.402099 | Val → total: 0.107304, cf: 0.078511, deriv: 0.575869\n",
            "[Epoch 58] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 58: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 58 → Current LR: 2.955196e-05\n",
            "Epoch [58/500] Train → total: 0.068421, cf: 0.048656, deriv: 0.395291 | Val → total: 0.102282, cf: 0.073611, deriv: 0.573426\n",
            "[Epoch 59] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 59: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 59 → Current LR: 3.000000e-04\n",
            "Epoch [59/500] Train → total: 0.064874, cf: 0.045482, deriv: 0.387841 | Val → total: 0.116184, cf: 0.085244, deriv: 0.618816\n",
            "[Epoch 60] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 60: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 60 → Current LR: 2.714480e-04\n",
            "Epoch [60/500] Train → total: 0.075845, cf: 0.056605, deriv: 0.384800 | Val → total: 0.134363, cf: 0.105514, deriv: 0.576972\n",
            "[Epoch 61] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 61: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 61 → Current LR: 1.966980e-04\n",
            "Epoch [61/500] Train → total: 0.076925, cf: 0.056759, deriv: 0.403305 | Val → total: 0.086234, cf: 0.061188, deriv: 0.500925\n",
            "[Epoch 62] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 62: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 62 → Current LR: 1.043020e-04\n",
            "Epoch [62/500] Train → total: 0.071856, cf: 0.051811, deriv: 0.400886 | Val → total: 0.085111, cf: 0.059365, deriv: 0.514930\n",
            "[Epoch 63] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 63: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 63 → Current LR: 2.955196e-05\n",
            "Epoch [63/500] Train → total: 0.066175, cf: 0.047160, deriv: 0.380294 | Val → total: 0.113727, cf: 0.083549, deriv: 0.603572\n",
            "[Epoch 64] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 64: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 64 → Current LR: 3.000000e-04\n",
            "Epoch [64/500] Train → total: 0.062919, cf: 0.043777, deriv: 0.382829 | Val → total: 0.110733, cf: 0.080706, deriv: 0.600535\n",
            "[Epoch 65] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 65: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 65 → Current LR: 2.714480e-04\n",
            "Epoch [65/500] Train → total: 0.075182, cf: 0.055445, deriv: 0.394726 | Val → total: 0.113663, cf: 0.086599, deriv: 0.541277\n",
            "[Epoch 66] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 66: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 66 → Current LR: 1.966980e-04\n",
            "Epoch [66/500] Train → total: 0.076338, cf: 0.056318, deriv: 0.400399 | Val → total: 0.099003, cf: 0.071836, deriv: 0.543340\n",
            "[Epoch 67] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 67: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 67 → Current LR: 1.043020e-04\n",
            "Epoch [67/500] Train → total: 0.069971, cf: 0.050477, deriv: 0.389880 | Val → total: 0.069963, cf: 0.047497, deriv: 0.449308\n",
            "[Epoch 68] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 68: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 68 → Current LR: 2.955196e-05\n",
            "Epoch [68/500] Train → total: 0.065912, cf: 0.046341, deriv: 0.391420 | Val → total: 0.079456, cf: 0.054907, deriv: 0.490981\n",
            "[Epoch 69] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 69: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 69 → Current LR: 3.000000e-04\n",
            "Epoch [69/500] Train → total: 0.061421, cf: 0.042296, deriv: 0.382495 | Val → total: 0.095183, cf: 0.066526, deriv: 0.573131\n",
            "[Epoch 70] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 70: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 70 → Current LR: 2.714480e-04\n",
            "Epoch [70/500] Train → total: 0.075800, cf: 0.056552, deriv: 0.384963 | Val → total: 0.099637, cf: 0.071826, deriv: 0.556223\n",
            "[Epoch 71] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 71: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 71 → Current LR: 1.966980e-04\n",
            "Epoch [71/500] Train → total: 0.071093, cf: 0.051970, deriv: 0.382466 | Val → total: 0.109676, cf: 0.082370, deriv: 0.546101\n",
            "[Epoch 72] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 72: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 72 → Current LR: 1.043020e-04\n",
            "Epoch [72/500] Train → total: 0.068930, cf: 0.049662, deriv: 0.385358 | Val → total: 0.095173, cf: 0.069144, deriv: 0.520580\n",
            "[Epoch 73] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 73: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 73 → Current LR: 2.955196e-05\n",
            "Epoch [73/500] Train → total: 0.062954, cf: 0.043942, deriv: 0.380233 | Val → total: 0.095550, cf: 0.068756, deriv: 0.535880\n",
            "[Epoch 74] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 74: 100%|██████████| 160/160 [03:46<00:00,  1.41s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 74 → Current LR: 3.000000e-04\n",
            "Epoch [74/500] Train → total: 0.059853, cf: 0.040564, deriv: 0.385779 | Val → total: 0.088388, cf: 0.060453, deriv: 0.558694\n",
            "[Epoch 75] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 75: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 75 → Current LR: 2.714480e-04\n",
            "Epoch [75/500] Train → total: 0.070635, cf: 0.051666, deriv: 0.379376 | Val → total: 0.132058, cf: 0.102159, deriv: 0.597979\n",
            "[Epoch 76] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 76: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 76 → Current LR: 1.966980e-04\n",
            "Epoch [76/500] Train → total: 0.069965, cf: 0.050394, deriv: 0.391429 | Val → total: 0.093030, cf: 0.067321, deriv: 0.514178\n",
            "[Epoch 77] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 77: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 77 → Current LR: 1.043020e-04\n",
            "Epoch [77/500] Train → total: 0.065620, cf: 0.046808, deriv: 0.376240 | Val → total: 0.064268, cf: 0.041558, deriv: 0.454208\n",
            "[Epoch 78] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 78: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 78 → Current LR: 2.955196e-05\n",
            "Epoch [78/500] Train → total: 0.061852, cf: 0.042520, deriv: 0.386645 | Val → total: 0.096442, cf: 0.069854, deriv: 0.531761\n",
            "[Epoch 79] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 79: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 79 → Current LR: 3.000000e-04\n",
            "Epoch [79/500] Train → total: 0.059179, cf: 0.039956, deriv: 0.384453 | Val → total: 0.063088, cf: 0.039588, deriv: 0.469997\n",
            "[Epoch 80] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 80: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 80 → Current LR: 2.714480e-04\n",
            "Epoch [80/500] Train → total: 0.070464, cf: 0.050952, deriv: 0.390242 | Val → total: 0.057807, cf: 0.035344, deriv: 0.449252\n",
            "[Epoch 81] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 81: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 81 → Current LR: 1.966980e-04\n",
            "Epoch [81/500] Train → total: 0.067688, cf: 0.048666, deriv: 0.380433 | Val → total: 0.082522, cf: 0.057699, deriv: 0.496454\n",
            "[Epoch 82] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 82: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 82 → Current LR: 1.043020e-04\n",
            "Epoch [82/500] Train → total: 0.062705, cf: 0.044118, deriv: 0.371744 | Val → total: 0.066359, cf: 0.044921, deriv: 0.428749\n",
            "[Epoch 83] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 83: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 83 → Current LR: 2.955196e-05\n",
            "Epoch [83/500] Train → total: 0.060768, cf: 0.041720, deriv: 0.380974 | Val → total: 0.079300, cf: 0.053104, deriv: 0.523921\n",
            "[Epoch 84] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 84: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 84 → Current LR: 3.000000e-04\n",
            "Epoch [84/500] Train → total: 0.057951, cf: 0.038583, deriv: 0.387358 | Val → total: 0.069850, cf: 0.045753, deriv: 0.481926\n",
            "[Epoch 85] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 85: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 85 → Current LR: 2.714480e-04\n",
            "Epoch [85/500] Train → total: 0.069112, cf: 0.050247, deriv: 0.377290 | Val → total: 0.074079, cf: 0.049134, deriv: 0.498904\n",
            "[Epoch 86] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 86: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 86 → Current LR: 1.966980e-04\n",
            "Epoch [86/500] Train → total: 0.066366, cf: 0.047232, deriv: 0.382674 | Val → total: 0.059512, cf: 0.037984, deriv: 0.430564\n",
            "[Epoch 87] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 87: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 87 → Current LR: 1.043020e-04\n",
            "Epoch [87/500] Train → total: 0.060597, cf: 0.041753, deriv: 0.376881 | Val → total: 0.059953, cf: 0.037370, deriv: 0.451649\n",
            "[Epoch 88] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 88: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 88 → Current LR: 2.955196e-05\n",
            "Epoch [88/500] Train → total: 0.057823, cf: 0.038460, deriv: 0.387260 | Val → total: 0.056486, cf: 0.034703, deriv: 0.435658\n",
            "[Epoch 89] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 89: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 89 → Current LR: 3.000000e-04\n",
            "Epoch [89/500] Train → total: 0.054388, cf: 0.035362, deriv: 0.380526 | Val → total: 0.046754, cf: 0.025072, deriv: 0.433647\n",
            "[Epoch 90] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 90: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 90 → Current LR: 2.714480e-04\n",
            "Epoch [90/500] Train → total: 0.065602, cf: 0.046615, deriv: 0.379752 | Val → total: 0.074651, cf: 0.049709, deriv: 0.498840\n",
            "[Epoch 91] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 91: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 91 → Current LR: 1.966980e-04\n",
            "Epoch [91/500] Train → total: 0.063303, cf: 0.044186, deriv: 0.382334 | Val → total: 0.054295, cf: 0.032932, deriv: 0.427251\n",
            "[Epoch 92] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 92: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 92 → Current LR: 1.043020e-04\n",
            "Epoch [92/500] Train → total: 0.058757, cf: 0.039604, deriv: 0.383067 | Val → total: 0.043928, cf: 0.023716, deriv: 0.404243\n",
            "[Epoch 93] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 93: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 93 → Current LR: 2.955196e-05\n",
            "Epoch [93/500] Train → total: 0.053697, cf: 0.035020, deriv: 0.373537 | Val → total: 0.039213, cf: 0.019489, deriv: 0.394468\n",
            "[Epoch 94] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 94: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 94 → Current LR: 3.000000e-04\n",
            "Epoch [94/500] Train → total: 0.051934, cf: 0.032961, deriv: 0.379458 | Val → total: 0.036565, cf: 0.016127, deriv: 0.408757\n",
            "[Epoch 95] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 95: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 95 → Current LR: 2.714480e-04\n",
            "Epoch [95/500] Train → total: 0.064317, cf: 0.044789, deriv: 0.390563 | Val → total: 0.058095, cf: 0.039192, deriv: 0.378067\n",
            "[Epoch 96] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 96: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 96 → Current LR: 1.966980e-04\n",
            "Epoch [96/500] Train → total: 0.060079, cf: 0.040733, deriv: 0.386929 | Val → total: 0.035658, cf: 0.017904, deriv: 0.355090\n",
            "[Epoch 97] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 97: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 97 → Current LR: 1.043020e-04\n",
            "Epoch [97/500] Train → total: 0.056242, cf: 0.038128, deriv: 0.362270 | Val → total: 0.039435, cf: 0.020580, deriv: 0.377116\n",
            "[Epoch 98] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 98: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 98 → Current LR: 2.955196e-05\n",
            "Epoch [98/500] Train → total: 0.051389, cf: 0.032854, deriv: 0.370699 | Val → total: 0.033474, cf: 0.014940, deriv: 0.370689\n",
            "[Epoch 99] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 99: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 99 → Current LR: 3.000000e-04\n",
            "Epoch [99/500] Train → total: 0.048925, cf: 0.030538, deriv: 0.367742 | Val → total: 0.029793, cf: 0.011829, deriv: 0.359275\n",
            "🔄 Model checkpoint saved at epoch 100: /content/drive/MyDrive/Ucl/checkpoints/deeponet_model-epoch100.pt\n",
            "[Epoch 100] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 100: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100 → Current LR: 2.714480e-04\n",
            "Epoch [100/500] Train → total: 0.058203, cf: 0.039148, deriv: 0.381091 | Val → total: 0.038748, cf: 0.020442, deriv: 0.366112\n",
            "[Epoch 101] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 101: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 101 → Current LR: 1.966980e-04\n",
            "Epoch [101/500] Train → total: 0.055230, cf: 0.036802, deriv: 0.368559 | Val → total: 0.046796, cf: 0.026481, deriv: 0.406300\n",
            "[Epoch 102] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 102: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 102 → Current LR: 1.043020e-04\n",
            "Epoch [102/500] Train → total: 0.054003, cf: 0.034873, deriv: 0.382592 | Val → total: 0.038185, cf: 0.020972, deriv: 0.344264\n",
            "[Epoch 103] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 103: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 103 → Current LR: 2.955196e-05\n",
            "Epoch [103/500] Train → total: 0.048672, cf: 0.030007, deriv: 0.373300 | Val → total: 0.029935, cf: 0.013072, deriv: 0.337271\n",
            "[Epoch 104] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 104: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 104 → Current LR: 3.000000e-04\n",
            "Epoch [104/500] Train → total: 0.047392, cf: 0.028618, deriv: 0.375465 | Val → total: 0.029734, cf: 0.011716, deriv: 0.360355\n",
            "[Epoch 105] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 105: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 105 → Current LR: 2.714480e-04\n",
            "Epoch [105/500] Train → total: 0.058809, cf: 0.040132, deriv: 0.373543 | Val → total: 0.040449, cf: 0.020801, deriv: 0.392972\n",
            "[Epoch 106] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 106: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 106 → Current LR: 1.966980e-04\n",
            "Epoch [106/500] Train → total: 0.056543, cf: 0.037414, deriv: 0.382589 | Val → total: 0.039708, cf: 0.020638, deriv: 0.381413\n",
            "[Epoch 107] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 107: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 107 → Current LR: 1.043020e-04\n",
            "Epoch [107/500] Train → total: 0.050593, cf: 0.031959, deriv: 0.372668 | Val → total: 0.030809, cf: 0.012954, deriv: 0.357094\n",
            "[Epoch 108] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 108: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 108 → Current LR: 2.955196e-05\n",
            "Epoch [108/500] Train → total: 0.047754, cf: 0.029651, deriv: 0.362067 | Val → total: 0.031876, cf: 0.013628, deriv: 0.364973\n",
            "[Epoch 109] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 109: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 109 → Current LR: 3.000000e-04\n",
            "Epoch [109/500] Train → total: 0.045360, cf: 0.027126, deriv: 0.364681 | Val → total: 0.029569, cf: 0.012033, deriv: 0.350717\n",
            "[Epoch 110] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 110: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 110 → Current LR: 2.714480e-04\n",
            "Epoch [110/500] Train → total: 0.056035, cf: 0.037595, deriv: 0.368800 | Val → total: 0.051069, cf: 0.032034, deriv: 0.380707\n",
            "[Epoch 111] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 111: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 111 → Current LR: 1.966980e-04\n",
            "Epoch [111/500] Train → total: 0.054744, cf: 0.036111, deriv: 0.372662 | Val → total: 0.041929, cf: 0.025714, deriv: 0.324298\n",
            "[Epoch 112] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 112: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 112 → Current LR: 1.043020e-04\n",
            "Epoch [112/500] Train → total: 0.049533, cf: 0.030787, deriv: 0.374929 | Val → total: 0.034783, cf: 0.016481, deriv: 0.366036\n",
            "[Epoch 113] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 113: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 113 → Current LR: 2.955196e-05\n",
            "Epoch [113/500] Train → total: 0.046431, cf: 0.027781, deriv: 0.372984 | Val → total: 0.030737, cf: 0.013567, deriv: 0.343406\n",
            "[Epoch 114] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 114: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 114 → Current LR: 3.000000e-04\n",
            "Epoch [114/500] Train → total: 0.044715, cf: 0.026344, deriv: 0.367410 | Val → total: 0.026774, cf: 0.010198, deriv: 0.331527\n",
            "[Epoch 115] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 115: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 115 → Current LR: 2.714480e-04\n",
            "Epoch [115/500] Train → total: 0.051969, cf: 0.033815, deriv: 0.363080 | Val → total: 0.034169, cf: 0.016639, deriv: 0.350616\n",
            "[Epoch 116] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 116: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 116 → Current LR: 1.966980e-04\n",
            "Epoch [116/500] Train → total: 0.052352, cf: 0.033793, deriv: 0.371184 | Val → total: 0.035695, cf: 0.017702, deriv: 0.359851\n",
            "[Epoch 117] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 117: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 117 → Current LR: 1.043020e-04\n",
            "Epoch [117/500] Train → total: 0.049141, cf: 0.030716, deriv: 0.368498 | Val → total: 0.034902, cf: 0.017613, deriv: 0.345767\n",
            "[Epoch 118] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 118: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 118 → Current LR: 2.955196e-05\n",
            "Epoch [118/500] Train → total: 0.046031, cf: 0.027186, deriv: 0.376892 | Val → total: 0.031795, cf: 0.014409, deriv: 0.347714\n",
            "[Epoch 119] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 119: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 119 → Current LR: 3.000000e-04\n",
            "Epoch [119/500] Train → total: 0.043698, cf: 0.025356, deriv: 0.366842 | Val → total: 0.027158, cf: 0.010412, deriv: 0.334906\n",
            "[Epoch 120] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 120: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 120 → Current LR: 2.714480e-04\n",
            "Epoch [120/500] Train → total: 0.051488, cf: 0.032780, deriv: 0.374170 | Val → total: 0.038529, cf: 0.021037, deriv: 0.349829\n",
            "[Epoch 121] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 121: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 121 → Current LR: 1.966980e-04\n",
            "Epoch [121/500] Train → total: 0.050162, cf: 0.031912, deriv: 0.364992 | Val → total: 0.036443, cf: 0.020073, deriv: 0.327404\n",
            "[Epoch 122] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 122: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 122 → Current LR: 1.043020e-04\n",
            "Epoch [122/500] Train → total: 0.047937, cf: 0.028893, deriv: 0.380878 | Val → total: 0.031413, cf: 0.013505, deriv: 0.358155\n",
            "[Epoch 123] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 123: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 123 → Current LR: 2.955196e-05\n",
            "Epoch [123/500] Train → total: 0.044638, cf: 0.025795, deriv: 0.376843 | Val → total: 0.028839, cf: 0.011015, deriv: 0.356480\n",
            "[Epoch 124] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 124: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 124 → Current LR: 3.000000e-04\n",
            "Epoch [124/500] Train → total: 0.042652, cf: 0.024288, deriv: 0.367290 | Val → total: 0.027902, cf: 0.010849, deriv: 0.341051\n",
            "[Epoch 125] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 125: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 125 → Current LR: 2.714480e-04\n",
            "Epoch [125/500] Train → total: 0.052467, cf: 0.033672, deriv: 0.375885 | Val → total: 0.031751, cf: 0.015865, deriv: 0.317711\n",
            "[Epoch 126] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 126: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 126 → Current LR: 1.966980e-04\n",
            "Epoch [126/500] Train → total: 0.047992, cf: 0.029155, deriv: 0.376735 | Val → total: 0.036459, cf: 0.019573, deriv: 0.337729\n",
            "[Epoch 127] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 127: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 127 → Current LR: 1.043020e-04\n",
            "Epoch [127/500] Train → total: 0.046289, cf: 0.028038, deriv: 0.365022 | Val → total: 0.033852, cf: 0.015282, deriv: 0.371402\n",
            "[Epoch 128] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 128: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 128 → Current LR: 2.955196e-05\n",
            "Epoch [128/500] Train → total: 0.043461, cf: 0.025118, deriv: 0.366860 | Val → total: 0.028596, cf: 0.012208, deriv: 0.327771\n",
            "[Epoch 129] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 129: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 129 → Current LR: 3.000000e-04\n",
            "Epoch [129/500] Train → total: 0.042382, cf: 0.023762, deriv: 0.372384 | Val → total: 0.028524, cf: 0.010664, deriv: 0.357212\n",
            "[Epoch 130] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 130: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 130 → Current LR: 2.714480e-04\n",
            "Epoch [130/500] Train → total: 0.050533, cf: 0.031809, deriv: 0.374483 | Val → total: 0.034696, cf: 0.016856, deriv: 0.356795\n",
            "[Epoch 131] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 131: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 131 → Current LR: 1.966980e-04\n",
            "Epoch [131/500] Train → total: 0.048556, cf: 0.029642, deriv: 0.378270 | Val → total: 0.035707, cf: 0.017189, deriv: 0.370363\n",
            "[Epoch 132] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 132: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 132 → Current LR: 1.043020e-04\n",
            "Epoch [132/500] Train → total: 0.046001, cf: 0.027713, deriv: 0.365751 | Val → total: 0.032620, cf: 0.014828, deriv: 0.355840\n",
            "[Epoch 133] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 133: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 133 → Current LR: 2.955196e-05\n",
            "Epoch [133/500] Train → total: 0.042847, cf: 0.024466, deriv: 0.367609 | Val → total: 0.029017, cf: 0.011758, deriv: 0.345188\n",
            "[Epoch 134] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 134: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 134 → Current LR: 3.000000e-04\n",
            "Epoch [134/500] Train → total: 0.041859, cf: 0.023176, deriv: 0.373656 | Val → total: 0.028511, cf: 0.010520, deriv: 0.359819\n",
            "[Epoch 135] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 135: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 135 → Current LR: 2.714480e-04\n",
            "Epoch [135/500] Train → total: 0.049668, cf: 0.031076, deriv: 0.371841 | Val → total: 0.038029, cf: 0.019933, deriv: 0.361918\n",
            "[Epoch 136] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 136: 100%|██████████| 160/160 [03:49<00:00,  1.43s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 136 → Current LR: 1.966980e-04\n",
            "Epoch [136/500] Train → total: 0.048498, cf: 0.029807, deriv: 0.373813 | Val → total: 0.033532, cf: 0.017956, deriv: 0.311522\n",
            "[Epoch 137] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 137: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 137 → Current LR: 1.043020e-04\n",
            "Epoch [137/500] Train → total: 0.045760, cf: 0.027714, deriv: 0.360920 | Val → total: 0.030333, cf: 0.012641, deriv: 0.353844\n",
            "[Epoch 138] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 138: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 138 → Current LR: 2.955196e-05\n",
            "Epoch [138/500] Train → total: 0.041364, cf: 0.023812, deriv: 0.351042 | Val → total: 0.029545, cf: 0.012206, deriv: 0.346792\n",
            "[Epoch 139] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 139: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 139 → Current LR: 3.000000e-04\n",
            "Epoch [139/500] Train → total: 0.039926, cf: 0.021954, deriv: 0.359435 | Val → total: 0.029164, cf: 0.011471, deriv: 0.353868\n",
            "[Epoch 140] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 140: 100%|██████████| 160/160 [03:49<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 140 → Current LR: 2.714480e-04\n",
            "Epoch [140/500] Train → total: 0.050034, cf: 0.031386, deriv: 0.372949 | Val → total: 0.043110, cf: 0.025447, deriv: 0.353261\n",
            "[Epoch 141] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 141: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 141 → Current LR: 1.966980e-04\n",
            "Epoch [141/500] Train → total: 0.047119, cf: 0.028847, deriv: 0.365433 | Val → total: 0.033996, cf: 0.016810, deriv: 0.343702\n",
            "[Epoch 142] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 142: 100%|██████████| 160/160 [03:48<00:00,  1.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 142 → Current LR: 1.043020e-04\n",
            "Epoch [142/500] Train → total: 0.044211, cf: 0.025793, deriv: 0.368362 | Val → total: 0.028994, cf: 0.011055, deriv: 0.358781\n",
            "[Epoch 143] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 143: 100%|██████████| 160/160 [03:50<00:00,  1.44s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 143 → Current LR: 2.955196e-05\n",
            "Epoch [143/500] Train → total: 0.041369, cf: 0.023210, deriv: 0.363182 | Val → total: 0.028619, cf: 0.011819, deriv: 0.335996\n",
            "[Epoch 144] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 144: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 144 → Current LR: 3.000000e-04\n",
            "Epoch [144/500] Train → total: 0.040538, cf: 0.022149, deriv: 0.367770 | Val → total: 0.027008, cf: 0.009904, deriv: 0.342072\n",
            "[Epoch 145] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 145: 100%|██████████| 160/160 [03:47<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 145 → Current LR: 2.714480e-04\n",
            "Epoch [145/500] Train → total: 0.047658, cf: 0.029378, deriv: 0.365590 | Val → total: 0.035859, cf: 0.017639, deriv: 0.364388\n",
            "[Epoch 146] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 146: 100%|██████████| 160/160 [03:46<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 146 → Current LR: 1.966980e-04\n",
            "Epoch [146/500] Train → total: 0.047994, cf: 0.029368, deriv: 0.372528 | Val → total: 0.036129, cf: 0.018237, deriv: 0.357841\n",
            "[Epoch 147] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 147: 100%|██████████| 160/160 [03:44<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 147 → Current LR: 1.043020e-04\n",
            "Epoch [147/500] Train → total: 0.043247, cf: 0.025057, deriv: 0.363796 | Val → total: 0.033794, cf: 0.016553, deriv: 0.344839\n",
            "[Epoch 148] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 148: 100%|██████████| 160/160 [03:44<00:00,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 148 → Current LR: 2.955196e-05\n",
            "Epoch [148/500] Train → total: 0.041746, cf: 0.022958, deriv: 0.375759 | Val → total: 0.029258, cf: 0.011049, deriv: 0.364177\n",
            "[Epoch 149] Updated branch/trunk scale → 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 149: 100%|██████████| 160/160 [03:44<00:00,  1.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 149 → Current LR: 3.000000e-04\n",
            "Epoch [149/500] Train → total: 0.039744, cf: 0.021526, deriv: 0.364375 | Val → total: 0.026495, cf: 0.009692, deriv: 0.336055\n",
            "🔄 Model checkpoint saved at epoch 150: /content/drive/MyDrive/Ucl/checkpoints/deeponet_model-epoch150.pt\n",
            "Early stopping triggered at epoch 149. Best val loss: 0.026774\n",
            "✅ Training finished, model saved as final_deeponet_model.pt\n",
            "✅ Scalers saved for consistent evaluation\n",
            "📁 Final model saved at: /content/drive/MyDrive/Ucl/final_deeponet_model.pt\n",
            "📁 Checkpoints saved at: /content/drive/MyDrive/Ucl/checkpoints/\n",
            "📁 Scalers saved at: /content/drive/MyDrive/Ucl/*_Scalar_Training.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m Couldn't retrieve and log Google Colab notebook content, reason: 'NoneType' object is not subscriptable\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : static_bug_5138\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/satyabratkumarsingh/option-portfolio-encoder-decoder/96ac953a751041d48e6b2089d00b1db8\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_cashflow_loss_batch [24000]   : (0.015873372554779053, 0.7178435325622559)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_cashflow_loss_epoch [150]     : (0.021525719726923853, 0.2982598843052983)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_derivative_loss_batch [24000] : (0.17953824996948242, 1.1214971542358398)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_derivative_loss_epoch [150]   : (0.35104154981672764, 0.7130866717547178)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_total_loss_batch [24000]      : (0.02929767034947872, 0.7567725777626038)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_total_loss_epoch [150]        : (0.03974445941857994, 0.33391421800479293)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_cashflow_loss_batch [6000]      : (0.007864557206630707, 0.33266380429267883)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_cashflow_loss_epoch [150]       : (0.009692420833744109, 0.2697137285023928)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_derivative_loss_batch [6000]    : (0.1886366754770279, 0.994316816329956)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_derivative_loss_epoch [150]     : (0.3115218494087458, 0.6861303858458996)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_total_loss_batch [6000]         : (0.01959189400076866, 0.3814318776130676)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_total_loss_epoch [150]          : (0.026495158206671476, 0.3040202476084232)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook_url : https://colab.research.google.com/notebook#fileId=1TLyI2xBR6xkR27v_-k4KOh5pR5G3xcWp\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size              : 128\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     early_stopping_patience : 35\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gradient_clip_norm      : 5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lambda_deriv            : 0.05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lambda_reg              : 0.0001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate           : 0.0003\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scheduler_T0            : 5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay            : 0.0001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import joblib  # Add joblib for saving scalers\n",
        "from comet_ml import start\n",
        "from tqdm import tqdm\n",
        "import os  # Add for directory creation\n",
        "\n",
        "# === HYPERPARAMETERS ===\n",
        "hidden_dim = 128\n",
        "latent_dim = 128\n",
        "batch_size = 128\n",
        "epochs = 500\n",
        "portfolio_feature_dim = 3\n",
        "PORT_LEN = 100\n",
        "PORT_SAMPLE_SIZE = 25600\n",
        "FEED_ST_LEN_EACH_PORT = 100\n",
        "\n",
        "# === COMET SETUP ===\n",
        "experiment = start(\n",
        "    api_key=\"iatWnXT4JyBtDQhn7OfgISQoF\",\n",
        "    project_name=\"option-portfolio-encoder-decoder\",\n",
        "    workspace=\"satyabratkumarsingh\"\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    FIXED: Corrected main training loop with proper validation computation.\n",
        "    Added scaler saving for consistent evaluation.\n",
        "    Added model saving every 50 epochs with -epoch{no} suffix.\n",
        "    \"\"\"\n",
        "    hparams = get_stable_hyperparameters()\n",
        "    experiment.log_parameters(hparams)\n",
        "\n",
        "    # === Model ===\n",
        "    model = OptimizedDeepONet(\n",
        "        portfolio_feature_dim=portfolio_feature_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        latent_dim=latent_dim,\n",
        "        use_enhanced_transformer=True\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # --- STEP 1: Raw dataset for scaler fitting ---\n",
        "    raw_dataset = OperatorDatasetStandardized(\n",
        "        num_samples=PORT_SAMPLE_SIZE,\n",
        "        min_portfolio_size=1,\n",
        "        max_portfolio_size=PORT_LEN,\n",
        "        num_samples_S_T=FEED_ST_LEN_EACH_PORT,\n",
        "        is_fitting_mode=True\n",
        "    )\n",
        "\n",
        "    # Train/Val split\n",
        "    train_size = int(0.8 * len(raw_dataset))\n",
        "    val_size = len(raw_dataset) - train_size\n",
        "    val_size = (val_size // batch_size) * batch_size\n",
        "    train_size = len(raw_dataset) - val_size\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "    train_dataset, val_dataset = random_split(raw_dataset, [train_size, val_size])\n",
        "    train_loader_fitting = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Fit scalers\n",
        "    print(\"Fitting scalers on training data...\")\n",
        "    K_scalar, S_T_scalar = fit_K_ST_scalers(train_loader_fitting)\n",
        "    cashflow_scaler = fit_cashflow_scaler(train_loader_fitting)\n",
        "\n",
        "    # === SAVE SCALERS ===\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/Ucl/\"\n",
        "    print(\"Saving scalers for evaluation consistency...\")\n",
        "\n",
        "    # Create checkpoint directory for periodic model saves\n",
        "    checkpoint_dir = DRIVE_PATH + \"checkpoints/\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Save scalers with consistent naming\n",
        "        joblib.dump(K_scalar, DRIVE_PATH + \"K_Scalar_Training.pkl\")\n",
        "        joblib.dump(S_T_scalar, DRIVE_PATH + \"S_T_Scalar_Training.pkl\")\n",
        "        joblib.dump(cashflow_scaler, DRIVE_PATH + \"Cashflow_Scalar_Training.pkl\")\n",
        "        print(\"✅ Successfully saved all training scalers\")\n",
        "\n",
        "        # Print scaler statistics for debugging\n",
        "        print(f\"📊 Scaler Statistics:\")\n",
        "        print(f\"K_scaler - mean: {K_scalar.mean_[0]:.4f}, std: {np.sqrt(K_scalar.var_[0]):.4f}\")\n",
        "        print(f\"S_T_scaler - mean: {S_T_scalar.mean_[0]:.4f}, std: {np.sqrt(S_T_scalar.var_[0]):.4f}\")\n",
        "        print(f\"Cashflow_scaler - mean: {cashflow_scaler.mean_[0]:.4f}, std: {np.sqrt(cashflow_scaler.var_[0]):.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error saving scalers: {e}\")\n",
        "        print(\"Continuing with training but evaluation may be inconsistent...\")\n",
        "\n",
        "    # --- STEP 2: Normalized dataset ---\n",
        "    normalized_dataset = OperatorDatasetStandardized(\n",
        "        num_samples=PORT_SAMPLE_SIZE,\n",
        "        min_portfolio_size=1,  # Add missing parameter for consistency\n",
        "        max_portfolio_size=PORT_LEN,\n",
        "        num_samples_S_T=FEED_ST_LEN_EACH_PORT,\n",
        "        K_scaler=K_scalar,\n",
        "        S_T_scaler=S_T_scalar,\n",
        "        cashflow_scaler=cashflow_scaler,\n",
        "        is_fitting_mode=False\n",
        "    )\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "    train_dataset, val_dataset = random_split(normalized_dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # === Trainer ===\n",
        "    trainer = OptimizedTrainer(\n",
        "        model,\n",
        "        device=DEVICE,\n",
        "        learning_rate=hparams[\"learning_rate\"],\n",
        "        lambda_deriv_weight=hparams[\"lambda_deriv\"],\n",
        "        weight_decay=hparams[\"weight_decay\"],\n",
        "        monitor_gradients=True,\n",
        "        grad_log_threshold=5.0,\n",
        "        scale_warmup_epochs=hparams[\"scheduler_T0\"],\n",
        "        initial_scale=0.05,\n",
        "        final_scale=1.0\n",
        "    )\n",
        "\n",
        "    early_stopper = ExtendedEarlyStopping(\n",
        "        patience=hparams[\"early_stopping_patience\"],\n",
        "        min_delta=0.001,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # === Training loop ===\n",
        "    for epoch in range(epochs):\n",
        "        trainer.update_scale(epoch)\n",
        "        model.train()\n",
        "\n",
        "        # --- Training Loop ---\n",
        "        train_total_losses, train_cf_losses, train_deriv_losses = [], [], []\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch}\")):\n",
        "            portfolio = batch[\"portfolio\"].to(DEVICE)\n",
        "            mask = batch[\"mask\"].to(DEVICE)\n",
        "            S_T = batch[\"S_T\"].to(DEVICE)\n",
        "            cashflow = batch[\"cashflow\"].to(DEVICE)\n",
        "            derivative = batch[\"derivative\"].to(DEVICE)\n",
        "\n",
        "            # CORRECT: Training step computes and returns training losses\n",
        "            total, cf, deriv = trainer.train_step(\n",
        "                portfolio,\n",
        "                S_T.clone().detach().requires_grad_(True),\n",
        "                cashflow,\n",
        "                derivative,\n",
        "                mask\n",
        "            )\n",
        "\n",
        "            train_total_losses.append(total)\n",
        "            train_cf_losses.append(cf)\n",
        "            train_deriv_losses.append(deriv)\n",
        "\n",
        "            # --- Batch-level logging ---\n",
        "            global_step = epoch * len(train_loader) + batch_idx\n",
        "            experiment.log_metric(\"train_total_loss_batch\", total, step=global_step)\n",
        "            experiment.log_metric(\"train_cashflow_loss_batch\", cf, step=global_step)\n",
        "            experiment.log_metric(\"train_derivative_loss_batch\", deriv, step=global_step)\n",
        "\n",
        "        # --- Epoch-level metrics ---\n",
        "        avg_train_total = np.mean(train_total_losses)\n",
        "        avg_train_cf = np.mean(train_cf_losses)\n",
        "        avg_train_deriv = np.mean(train_deriv_losses)\n",
        "\n",
        "        experiment.log_metric(\"train_total_loss_epoch\", avg_train_total, step=epoch)\n",
        "        experiment.log_metric(\"train_cashflow_loss_epoch\", avg_train_cf, step=epoch)\n",
        "        experiment.log_metric(\"train_derivative_loss_epoch\", avg_train_deriv, step=epoch)\n",
        "\n",
        "        # --- FIXED: Validation Loop ---\n",
        "        model.eval()\n",
        "        val_total_losses, val_cf_losses, val_deriv_losses = [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                portfolio = batch[\"portfolio\"].to(DEVICE)\n",
        "                mask = batch[\"mask\"].to(DEVICE)\n",
        "                S_T = batch[\"S_T\"].to(DEVICE)\n",
        "                cashflow = batch[\"cashflow\"].to(DEVICE)\n",
        "                derivative = batch[\"derivative\"].to(DEVICE)\n",
        "\n",
        "                # FIXED: Actually compute validation losses using val_step\n",
        "                total, cf, deriv = trainer.val_step(\n",
        "                    portfolio, S_T, cashflow, derivative, mask\n",
        "                )\n",
        "\n",
        "                val_total_losses.append(total)\n",
        "                val_cf_losses.append(cf)\n",
        "                val_deriv_losses.append(deriv)\n",
        "\n",
        "                # --- Batch-level validation logging ---\n",
        "                global_step = epoch * len(val_loader) + batch_idx\n",
        "                experiment.log_metric(\"val_total_loss_batch\", total, step=global_step)\n",
        "                experiment.log_metric(\"val_cashflow_loss_batch\", cf, step=global_step)\n",
        "                experiment.log_metric(\"val_derivative_loss_batch\", deriv, step=global_step)\n",
        "\n",
        "        # Step the scheduler once per epoch\n",
        "        trainer.step_scheduler_epoch()\n",
        "\n",
        "        current_lr = trainer.optimizer.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch} → Current LR: {current_lr:.6e}\")\n",
        "\n",
        "        # --- Epoch-level validation metrics ---\n",
        "        avg_val_total = np.mean(val_total_losses)\n",
        "        avg_val_cf = np.mean(val_cf_losses)\n",
        "        avg_val_deriv = np.mean(val_deriv_losses)\n",
        "\n",
        "        experiment.log_metric(\"val_total_loss_epoch\", avg_val_total, step=epoch)\n",
        "        experiment.log_metric(\"val_cashflow_loss_epoch\", avg_val_cf, step=epoch)\n",
        "        experiment.log_metric(\"val_derivative_loss_epoch\", avg_val_deriv, step=epoch)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch}/{epochs}] \"\n",
        "            f\"Train → total: {avg_train_total:.6f}, cf: {avg_train_cf:.6f}, deriv: {avg_train_deriv:.6f} | \"\n",
        "            f\"Val → total: {avg_val_total:.6f}, cf: {avg_val_cf:.6f}, deriv: {avg_val_deriv:.6f}\"\n",
        "        )\n",
        "\n",
        "        # === SAVE MODEL EVERY 50 EPOCHS ===\n",
        "        if (epoch + 1) % 50 == 0:  # epoch starts from 0, so epoch+1 for proper numbering\n",
        "            checkpoint_path = checkpoint_dir + f\"deeponet_model-epoch{epoch + 1}.pt\"\n",
        "\n",
        "            # Save model checkpoint using the reusable function\n",
        "            save_model_checkpoint(\n",
        "                model=model,\n",
        "                save_path=checkpoint_path,\n",
        "                epoch=epoch + 1,\n",
        "                optimizer=trainer.optimizer,\n",
        "                scheduler=trainer.scheduler if hasattr(trainer, 'scheduler') else None,\n",
        "                train_loss=avg_train_total,\n",
        "                val_loss=avg_val_total\n",
        "            )\n",
        "\n",
        "            print(f\"🔄 Model checkpoint saved at epoch {epoch + 1}: {checkpoint_path}\")\n",
        "\n",
        "        # --- Early Stopping Check ---\n",
        "        stop = early_stopper(avg_val_total, model)\n",
        "        if stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch}. Best val loss: {early_stopper.best:.6f}\")\n",
        "            break\n",
        "\n",
        "    # === SAVE FINAL MODEL WITH METADATA ===\n",
        "    save_path = DRIVE_PATH + \"final_deeponet_model.pt\"\n",
        "\n",
        "    # Save final model using the reusable function\n",
        "    save_model_checkpoint(model=model, save_path=save_path)\n",
        "\n",
        "    print(\"✅ Training finished, model saved as final_deeponet_model.pt\")\n",
        "    print(\"✅ Scalers saved for consistent evaluation\")\n",
        "    print(f\"📁 Final model saved at: {save_path}\")\n",
        "    print(f\"📁 Checkpoints saved at: {checkpoint_dir}\")\n",
        "    print(f\"📁 Scalers saved at: {DRIVE_PATH}*_Scalar_Training.pkl\")\n",
        "\n",
        "    experiment.end()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pGSJINJMwIl8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yblg0ag0gWNH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPNKGD0vcY1HFImL4GJB6KQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}