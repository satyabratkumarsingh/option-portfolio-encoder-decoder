{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGmoBHSzMhUBY+ffus2QZm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyabratkumarsingh/option-portfolio-encoder-decoder/blob/main/Advanced_Set_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJZFIbU9_YfV",
        "outputId": "076cfcef-6330-41af-f3b1-2c219678295f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting comet_ml\n",
            "  Downloading comet_ml-3.49.11-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n",
            "  Downloading dulwich-0.23.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting everett<3.2.0,>=1.0.1 (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (4.24.0)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (5.9.5)\n",
            "Collecting python-box<7.0.0 (from comet_ml)\n",
            "  Downloading python_box-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.32.3)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (13.9.4)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.32.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.11/dist-packages (from comet_ml) (3.20.1)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (1.17.2)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from comet_ml) (3.1.1)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.26.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.18.4->comet_ml) (2025.7.9)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet_ml) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema!=3.1.0,>=2.6.0->comet_ml) (4.14.1)\n",
            "Downloading comet_ml-3.49.11-py3-none-any.whl (727 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.1/727.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dulwich-0.23.2-cp311-cp311-manylinux_2_28_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading python_box-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: everett, python-box, dulwich, configobj, comet_ml\n",
            "  Attempting uninstall: python-box\n",
            "    Found existing installation: python-box 7.3.2\n",
            "    Uninstalling python-box-7.3.2:\n",
            "      Successfully uninstalled python-box-7.3.2\n",
            "Successfully installed comet_ml-3.49.11 configobj-5.0.9 dulwich-0.23.2 everett-3.1.0 python-box-6.1.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install comet_ml\n",
        "!pip install tqdm\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "l5A1HHxp_kiG",
        "outputId": "4b455cbe-8118-41b0-dbb5-7339d070e90f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def delete_file_from_drive(full_file_path):\n",
        "  if os.path.exists(full_file_path):\n",
        "      try:\n",
        "          os.remove(full_file_path)\n",
        "          print(f\"File '{full_file_path}' successfully deleted from Google Drive.\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error deleting file '{full_file_path}': {e}\")\n",
        "  else:\n",
        "      print(f\"File '{full_file_path}' not found at '{full_file_path}'.\")\n"
      ],
      "metadata": {
        "id": "fNUnLv8l_pSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import itertools\n",
        "from itertools import product\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc # For garbage collection\n",
        "\n",
        "# Parameters\n",
        "MU = 0.05\n",
        "SIGMA = 0.2\n",
        "T = 1.0 # Time to maturity\n",
        "NOISE_STD = 0.005\n",
        "MIN_PRICE_RANGE = 100\n",
        "MAX_PRICE_RANGE = 200\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def generate_option_prices_for_idx(idx, n, weights=None):\n",
        "    # Use numpy's default_rng for better random number generation and seeding\n",
        "    rng = np.random.default_rng(idx)\n",
        "    # Use torch.manual_seed for PyTorch operations\n",
        "    torch.manual_seed(idx)\n",
        "    if DEVICE.type == 'cuda':\n",
        "        torch.cuda.manual_seed_all(idx)\n",
        "\n",
        "    # Generate S_0\n",
        "    random_number = rng.integers(MIN_PRICE_RANGE, MAX_PRICE_RANGE + 1) # +1 because randint is inclusive\n",
        "    min_price = random_number\n",
        "    max_price = random_number + 5\n",
        "    S_0 = rng.uniform(min_price, max_price)\n",
        "\n",
        "    # Generate option types\n",
        "    option_types = rng.choice([\"call\", \"put\"], size=n)\n",
        "    option_types_numeric = np.where(option_types == \"call\", 1, 0).astype(np.float32) # Ensure float32\n",
        "\n",
        "    # Generate X_prices (strike prices)\n",
        "    K_prices = np.zeros(n, dtype=np.float32) # Ensure float32\n",
        "    for i in range(n):\n",
        "        K_prices[i] = S_0 * rng.uniform(0.90, 1.20)\n",
        "\n",
        "    # Generate or use weights\n",
        "    if weights is None:\n",
        "        # If weights are not provided, generate them using generate_combinatorial_weights_manageable\n",
        "        weight_sets = generate_combinatorial_weights_manageable(n)\n",
        "        weights_array = weight_sets[0]  # Use the first (and only) set of weights\n",
        "    else:\n",
        "        weights_array = np.array(weights, dtype=np.float32)  # Ensure float32\n",
        "\n",
        "    return K_prices, option_types_numeric, S_0, weights_array\n",
        "\n",
        "\n",
        "def generate_combinatorial_weights_manageable(n, base_weights=[-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75]):\n",
        "    weight_sets = []\n",
        "\n",
        "    # Handle the case where n < 2\n",
        "    if n < 2:\n",
        "        weights = np.zeros(n, dtype=np.float32)\n",
        "        if n == 1:\n",
        "            # If only one position, assign a long position (1.0)\n",
        "            weights[0] = 1.0\n",
        "        weight_sets.append(weights)\n",
        "        return weight_sets\n",
        "\n",
        "    # Generate a single portfolio: either one long or one short, and the rest from combinatorics\n",
        "    weights = np.zeros(n, dtype=np.float32)\n",
        "\n",
        "    # Randomly choose if we want a long or short portfolio\n",
        "    is_long = random.choice([True, False])\n",
        "\n",
        "    if is_long:\n",
        "        # Choose one position to be long (1.0)\n",
        "        long_idx = random.randint(0, n - 1)\n",
        "        weights[long_idx] = 1.0\n",
        "    else:\n",
        "        # Choose one position to be short (-1.0)\n",
        "        short_idx = random.randint(0, n - 1)\n",
        "        weights[short_idx] = -1.0\n",
        "\n",
        "    # Fill remaining positions with combinatorial weights from base_weights\n",
        "    remaining_positions = [i for i in range(n) if weights[i] == 0]  # Find positions not yet filled\n",
        "    combinatorics = np.random.choice(base_weights, size=len(remaining_positions), replace=True)\n",
        "\n",
        "    # Assign combinatorial weights to the remaining positions without normalization\n",
        "    weights[remaining_positions] = combinatorics\n",
        "\n",
        "    weight_sets.append(weights)\n",
        "\n",
        "    return weight_sets\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def black_scholes_delta(S, K, T, r, sigma, option_type):\n",
        "    \"\"\"\n",
        "    Computes the Black-Scholes delta for a call or put option.\n",
        "\n",
        "    Args:\n",
        "        S (Tensor): Spot price [any shape]\n",
        "        K (Tensor): Strike price [same shape as S or broadcastable]\n",
        "        T (float or Tensor): Time to maturity (scalar or broadcastable)\n",
        "        r (float or Tensor): Risk-free rate (scalar or broadcastable)\n",
        "        sigma (float or Tensor): Volatility (scalar or broadcastable)\n",
        "        option_type (Tensor): 1 for call, 0 for put [same shape as S]\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Delta of the option [same shape as S]\n",
        "    \"\"\"\n",
        "    eps = 1e-8  # Numerical stability for sqrt\n",
        "    device = S.device\n",
        "\n",
        "    T = torch.as_tensor(T, device=device, dtype=S.dtype)\n",
        "    r = torch.as_tensor(r, device=device, dtype=S.dtype)\n",
        "    sigma = torch.as_tensor(sigma, device=device, dtype=S.dtype)\n",
        "\n",
        "    d1 = (torch.log(S / K + eps) + (r + 0.5 * sigma ** 2) * T) / (sigma * torch.sqrt(T + eps))\n",
        "\n",
        "    # More stable, recommended: torch.special.ndtr(d1) (if available)\n",
        "    N_d1 = torch.distributions.Normal(0.0, 1.0).cdf(d1)\n",
        "\n",
        "    delta = torch.where(option_type == 1, N_d1, N_d1 - 1.0)\n",
        "    return delta\n",
        "\n",
        "def compute_cashflow(portfolio, S_T, weights):\n",
        "    strikes = portfolio[..., 0]\n",
        "    types = portfolio[..., 1]\n",
        "    weights = weights.to(S_T.device)\n",
        "\n",
        "    # Compute payoff-based cashflow as before\n",
        "    payoffs = torch.where(\n",
        "        types == 1,\n",
        "        torch.relu(S_T - strikes),\n",
        "        torch.relu(strikes - S_T)\n",
        "    )\n",
        "    weighted_payoffs = payoffs * weights\n",
        "    cashflow = weighted_payoffs.sum(dim=-1, keepdim=True)\n",
        "\n",
        "   # --- Continuous Derivative (Black-Scholes Delta) ---\n",
        "    delta = black_scholes_delta(\n",
        "        S_T, strikes, T=T, r=MU, sigma=SIGMA, option_type=types\n",
        "    )\n",
        "    weighted_delta = weights * delta\n",
        "    derivative = weighted_delta.sum(dim=-1, keepdim=True)\n",
        "\n",
        "    return cashflow.to(torch.float32), derivative.to(torch.float32)\n",
        "\n"
      ],
      "metadata": {
        "id": "g8r9orFZ_sLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "class OperatorDatasetStandarization(Dataset):\n",
        "\n",
        "    def __init__(self, num_samples, portfolio_size, num_samples_S_T,\n",
        "                 K_Scalar=None, S_T_scalar=None, cashflow_scaler=None,\n",
        "                 is_fitting_mode=False):\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "        self.portfolio_size = portfolio_size\n",
        "        self.num_samples_S_T = num_samples_S_T\n",
        "        self.is_fitting_mode = is_fitting_mode # Store the mode\n",
        "\n",
        "        # Load or generate K_Scalar and S_T_scalar\n",
        "        if not self.is_fitting_mode:\n",
        "          if K_Scalar is None or S_T_scalar is None or cashflow_scaler is None:\n",
        "              raise ValueError(\"K_Scalar, S_T_scalar, and cashflow_scaler must be explicitly provided.\")\n",
        "          else:\n",
        "            self.K_Scalar = K_Scalar\n",
        "            self.S_T_scalar = S_T_scalar\n",
        "            self.cashflow_scaler = cashflow_scaler\n",
        "        else:\n",
        "            self.K_Scalar = None\n",
        "            self.S_T_scalar = None\n",
        "            self.cashflow_scaler = None\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Generates and returns a single data sample (portfolio, S_T, cashflow, derivative).\n",
        "        This method is called by the DataLoader.\n",
        "        \"\"\"\n",
        "\n",
        "        K, option_types, S_0, weights = generate_option_prices_for_idx(\n",
        "            idx, self.portfolio_size\n",
        "        )\n",
        "\n",
        "        portfolio_features_tensor = torch.stack([\n",
        "            torch.tensor(K, dtype=torch.float32, device=DEVICE),\n",
        "            torch.tensor(option_types, dtype=torch.float32, device=DEVICE),\n",
        "            torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
        "        ], dim=-1)\n",
        "\n",
        "        weights_i = torch.tensor(weights, dtype=torch.float32, device=DEVICE)\n",
        "        K_i = torch.tensor(K, dtype=torch.float32, device=DEVICE)\n",
        "        S_0_i = torch.tensor(S_0, dtype=torch.float32, device=DEVICE)\n",
        "        Z = torch.clamp(torch.randn(self.num_samples_S_T, device=DEVICE), -3, 3)\n",
        "\n",
        "        S_T_i = S_0_i * torch.exp((MU - 0.5 * SIGMA**2) * T + SIGMA * torch.sqrt(torch.tensor(T, device=DEVICE)) * Z)\n",
        "        S_T_i += torch.randn_like(S_T_i, device=DEVICE) * (NOISE_STD * S_T_i)\n",
        "\n",
        "        # Normalization for K and S_T (always apply if scalers are present and not in fitting mode)\n",
        "        # Note: In fitting mode, K and S_T are still generated, but their *normalized* versions\n",
        "        # are not what we're collecting for the cashflow scaler.\n",
        "        if not self.is_fitting_mode:\n",
        "            K_i_cpu = K_i.reshape(-1, 1).cpu()\n",
        "            K_i_normalized = torch.tensor(self.K_Scalar.transform(K_i_cpu), dtype=torch.float32, device=DEVICE)\n",
        "            S_T_i_normalized = torch.from_numpy(self.S_T_scalar.transform(S_T_i.cpu().numpy().reshape(-1, 1))).to(DEVICE).squeeze()\n",
        "        else: # In fitting mode, just use raw K_i and S_T_i for generating raw cashflow\n",
        "            K_i_normalized = K_i # This won't be used for input features directly, but kept for clarity\n",
        "            S_T_i_normalized = S_T_i\n",
        "\n",
        "\n",
        "        # Compute cashflow and derivative (ALWAYS raw when generated in __getitem__)\n",
        "        cashflow_i_raw, derivative_i_raw = compute_cashflow(\n",
        "            portfolio_features_tensor.expand(self.num_samples_S_T, -1, -1),\n",
        "            S_T_i.unsqueeze(-1),\n",
        "            weights_i.expand(self.num_samples_S_T, -1)\n",
        "        )\n",
        "\n",
        "        # ===== CASHFLOW NORMALIZATION (Apply only if a scaler is provided and not in fitting mode) =====\n",
        "        if self.cashflow_scaler is not None and not self.is_fitting_mode:\n",
        "            # Convert to numpy on CPU for scaler, then back to tensor\n",
        "            cashflow_i_normalized_np = self.cashflow_scaler.transform(cashflow_i_raw.cpu().numpy().reshape(-1, 1))\n",
        "            cashflow_i_to_return = torch.from_numpy(cashflow_i_normalized_np).to(DEVICE).squeeze()\n",
        "        else:\n",
        "            # If no scaler or in fitting mode, return the raw cashflow\n",
        "            cashflow_i_to_return = cashflow_i_raw.squeeze()\n",
        "\n",
        "        # Update portfolio_i_normalized with normalized K_i if not in fitting mode\n",
        "        if not self.is_fitting_mode:\n",
        "            portfolio_i_normalized = portfolio_features_tensor.clone()\n",
        "            portfolio_i_normalized[:, 0] = K_i_normalized.squeeze()\n",
        "        else: # In fitting mode, just return original K in portfolio_features_tensor for now\n",
        "            portfolio_i_normalized = portfolio_features_tensor.clone()\n",
        "\n",
        "\n",
        "        return portfolio_i_normalized, S_T_i_normalized,  cashflow_i_to_return, derivative_i_raw.squeeze()\n"
      ],
      "metadata": {
        "id": "5uf3nsG7_0MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit ST and K\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/Ucl/\"\n",
        "K_SCALAR_FILE = os.path.join(DRIVE_PATH, 'K_Scalar_Advanced.pkl')\n",
        "ST_SCALAR_FILE = os.path.join(DRIVE_PATH, 'S_T_Scalar_Advanced.pkl')\n",
        "CASHFLOW_SCALAR_FILE = os.path.join(DRIVE_PATH, 'Cashflow_Scalar_Advanced.pkl')\n",
        "\n",
        "\n",
        "\n",
        "def fit_K_ST_scalers(train_loader, save_path_K=K_SCALAR_FILE, save_path_ST=ST_SCALAR_FILE):\n",
        "    print(\"Fitting K and S_T scalers from training set...\")\n",
        "    all_K = []\n",
        "    all_S_T = []\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Collecting K and S_T for scalers\"):\n",
        "        portfolio_real, s_t_real, _, _ = batch\n",
        "\n",
        "        # K is the first column of the portfolio features\n",
        "        K_real = portfolio_real[:, :, 0].cpu().numpy().reshape(-1, 1)\n",
        "        S_T_real = s_t_real.cpu().numpy().reshape(-1, 1)\n",
        "\n",
        "        all_K.append(K_real)\n",
        "        all_S_T.append(S_T_real)\n",
        "\n",
        "    K_all_np = np.concatenate(all_K, axis=0)\n",
        "    S_T_all_np = np.concatenate(all_S_T, axis=0)\n",
        "\n",
        "    K_scalar = StandardScaler()\n",
        "    K_scalar.fit(K_all_np)\n",
        "    joblib.dump(K_scalar, save_path_K)\n",
        "\n",
        "    S_T_scalar = StandardScaler()\n",
        "    S_T_scalar.fit(S_T_all_np)\n",
        "    joblib.dump(S_T_scalar, save_path_ST)\n",
        "\n",
        "    print(f\"✅ Saved K scalar to: {save_path_K}\")\n",
        "    print(f\"✅ Saved S_T scalar to: {save_path_ST}\")\n",
        "    print(f\"K mean: {K_scalar.mean_[0]:.4f}, std: {K_scalar.scale_[0]:.4f}\")\n",
        "    print(f\"S_T mean: {S_T_scalar.mean_[0]:.4f}, std: {S_T_scalar.scale_[0]:.4f}\")\n",
        "\n",
        "    return K_scalar, S_T_scalar\n",
        "\n",
        "def fit_cashflow_scaler(train_loader, save_path=CASHFLOW_SCALAR_FILE):\n",
        "    import numpy as np\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import joblib\n",
        "\n",
        "    all_cashflows = []\n",
        "\n",
        "    for portfolio, s_t, cashflow, _ in tqdm(train_loader, desc=\"Fitting Cashflow Scaler\"):\n",
        "        cashflow = cashflow.detach().cpu().numpy().reshape(-1, 1)\n",
        "        all_cashflows.append(cashflow)\n",
        "\n",
        "    cashflows_np = np.concatenate(all_cashflows, axis=0)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(cashflows_np)\n",
        "    joblib.dump(scaler, save_path)\n",
        "\n",
        "    print(f\"✅ Saved Cashflow Scaler to: {save_path}\")\n",
        "    print(f\"Cashflow Mean: {scaler.mean_[0]:.4f}, Std Dev: {scaler.scale_[0]:.4f}\")\n",
        "\n",
        "    return scaler\n",
        "\n"
      ],
      "metadata": {
        "id": "N0J4G7c8_2-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "FEED_FWD_DEPTH = 4\n",
        "\n",
        "\n",
        "class TrunkNet(nn.Module):\n",
        "    def __init__(self, input_dim=1, latent_dim=64, hidden_dim=128,\n",
        "                 num_layers=6, dropout_prob=0.1):\n",
        "        super(TrunkNet, self).__init__()\n",
        "\n",
        "        # Input projection\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_prob)\n",
        "        )\n",
        "\n",
        "        # Residual blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout_prob)\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.output_proj = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, S_T):\n",
        "        if S_T.dim() == 1:\n",
        "            S_T = S_T.unsqueeze(-1)\n",
        "        elif S_T.dim() == 2:\n",
        "            S_T = S_T.unsqueeze(-1)\n",
        "\n",
        "        x = self.input_proj(S_T)\n",
        "\n",
        "        # Residual connections\n",
        "        for block in self.blocks:\n",
        "            x = x + block(x)\n",
        "\n",
        "        return self.output_proj(x)\n",
        "\n",
        "\n",
        "class ISAB(nn.Module):\n",
        "    \"\"\"Induced Set Attention Block using PyTorch's MultiheadAttention\"\"\"\n",
        "    def __init__(self, d_model, num_heads, num_inds, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_inds = num_inds\n",
        "        self.inducing_points = nn.Parameter(torch.randn(num_inds, d_model))\n",
        "\n",
        "        # Use PyTorch's built-in MultiheadAttention\n",
        "        self.attention1 = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.attention2 = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.norm4 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ffn1 = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * FEED_FWD_DEPTH),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * FEED_FWD_DEPTH, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.ffn2 = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * FEED_FWD_DEPTH),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * FEED_FWD_DEPTH, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Expand inducing points for batch\n",
        "        I = self.inducing_points.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "\n",
        "        # First attention: I attends to X\n",
        "        attn_out1, _ = self.attention1(I, x, x)\n",
        "        I = self.norm1(I + attn_out1)\n",
        "\n",
        "        # Feed-forward on inducing points\n",
        "        ffn_out1 = self.ffn1(I)\n",
        "        I = self.norm2(I + ffn_out1)\n",
        "\n",
        "        # Second attention: X attends to I\n",
        "        attn_out2, _ = self.attention2(x, I, I)\n",
        "        x = self.norm3(x + attn_out2)\n",
        "\n",
        "        # Feed-forward on original input\n",
        "        ffn_out2 = self.ffn2(x)\n",
        "        x = self.norm4(x + ffn_out2)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PMA(nn.Module):\n",
        "    \"\"\"Pooling by Multihead Attention using PyTorch's MultiheadAttention\"\"\"\n",
        "    def __init__(self, d_model, num_heads, num_seeds, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_seeds = num_seeds\n",
        "        self.seed_vectors = nn.Parameter(torch.randn(num_seeds, d_model))\n",
        "\n",
        "        # Use PyTorch's built-in MultiheadAttention\n",
        "        self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * FEED_FWD_DEPTH),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * FEED_FWD_DEPTH, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Expand seed vectors for batch\n",
        "        S = self.seed_vectors.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "\n",
        "        # Attention: seeds attend to input\n",
        "        attn_out, _ = self.attention(S, x, x)\n",
        "        S = self.norm1(S + attn_out)\n",
        "\n",
        "        # Feed-forward\n",
        "        ffn_out = self.ffn(S)\n",
        "        S = self.norm2(S + ffn_out)\n",
        "\n",
        "        return S\n",
        "\n",
        "\n",
        "class SAB(nn.Module):\n",
        "    \"\"\"Set Attention Block using PyTorch's TransformerEncoderLayer\"\"\"\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=d_model * FEED_FWD_DEPTH,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transformer_layer(x)\n",
        "\n",
        "\n",
        "class EnhancedSetTransformerEncoder(nn.Module):\n",
        "    \"\"\"Set Transformer with ISAB and PMA for better generalization\"\"\"\n",
        "    def __init__(self, portfolio_feature_dim=3, latent_dim=128, hidden_dim=64,\n",
        "                 num_heads=2, dropout_prob=0.1, num_inds=32, num_seeds=1,\n",
        "                 use_isab=True, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ensure hidden_dim is divisible by num_heads\n",
        "        if hidden_dim % num_heads != 0:\n",
        "            hidden_dim = ((hidden_dim // num_heads) + 1) * num_heads\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.use_isab = use_isab\n",
        "\n",
        "        # Input projection\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(portfolio_feature_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_prob)\n",
        "        )\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder_layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            if use_isab:\n",
        "                self.encoder_layers.append(\n",
        "                    ISAB(hidden_dim, num_heads, num_inds, dropout_prob)\n",
        "                )\n",
        "            else:\n",
        "                self.encoder_layers.append(\n",
        "                    SAB(hidden_dim, num_heads, dropout_prob)\n",
        "                )\n",
        "\n",
        "        # Pooling layer\n",
        "        self.pooling = PMA(hidden_dim, num_heads, num_seeds, dropout_prob)\n",
        "\n",
        "        # Output projection\n",
        "        self.output_proj = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, portfolio):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            portfolio: [batch_size, portfolio_size, portfolio_feature_dim]\n",
        "        Returns:\n",
        "            [batch_size, latent_dim]\n",
        "        \"\"\"\n",
        "        # Input projection\n",
        "        x = self.input_proj(portfolio)  # [B, P, H]\n",
        "\n",
        "        # Encoder layers\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Pooling\n",
        "        x = self.pooling(x)  # [B, num_seeds, H]\n",
        "\n",
        "        # If we have multiple seeds, we can pool them further\n",
        "        if x.size(1) > 1:\n",
        "            x = x.mean(dim=1)  # [B, H]\n",
        "        else:\n",
        "            x = x.squeeze(1)  # [B, H]\n",
        "\n",
        "        # Final projection\n",
        "        out = self.output_proj(x)  # [B, latent_dim]\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class OptimizedSetTransformerEncoder(nn.Module):\n",
        "    \"\"\"Original Set Transformer using PyTorch's built-in TransformerEncoder\"\"\"\n",
        "    def __init__(self, portfolio_feature_dim=3, latent_dim=128, hidden_dim=64,\n",
        "                 num_layers=1, num_heads=2, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ensure hidden_dim is divisible by num_heads\n",
        "        if hidden_dim % num_heads != 0:\n",
        "            hidden_dim = ((hidden_dim // num_heads) + 1) * num_heads\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Input projection\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(portfolio_feature_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_prob)\n",
        "        )\n",
        "\n",
        "        # PyTorch's built-in TransformerEncoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * FEED_FWD_DEPTH,\n",
        "            dropout=dropout_prob,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "            enable_nested_tensor=False\n",
        "        )\n",
        "\n",
        "        self.output_proj = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, portfolio):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            portfolio: [batch_size, portfolio_size, portfolio_feature_dim]\n",
        "        Returns:\n",
        "            [batch_size, latent_dim]\n",
        "        \"\"\"\n",
        "        x = self.input_proj(portfolio)  # [B, P, H]\n",
        "\n",
        "        # PyTorch transformer expects [batch, seq, feature] with batch_first=True\n",
        "        x = self.transformer_encoder(x)  # [B, P, H]\n",
        "\n",
        "        pooled = x.mean(dim=1) + x.max(dim=1).values\n",
        "\n",
        "        # Final projection\n",
        "        out = self.output_proj(pooled)  # [B, latent_dim]\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class OptimizedDeepONet(nn.Module):\n",
        "    \"\"\"DeepONet with choice of Set Transformer architectures\"\"\"\n",
        "    def __init__(self, portfolio_feature_dim=3, hidden_dim=64, latent_dim=128,\n",
        "                 dropout_prob=0.1, num_heads=2, use_enhanced_transformer=True,\n",
        "                 num_inds=32, num_seeds=1):\n",
        "        super().__init__()\n",
        "\n",
        "        if hidden_dim % num_heads != 0:\n",
        "            recommended = ((hidden_dim // num_heads) + 1) * num_heads\n",
        "            raise ValueError(\n",
        "                f\"hidden_dim ({hidden_dim}) must be divisible by num_heads ({num_heads}). \"\n",
        "                f\"Try hidden_dim={recommended}\"\n",
        "            )\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Choose branch network architecture\n",
        "        if use_enhanced_transformer:\n",
        "            self.branch_net = EnhancedSetTransformerEncoder(\n",
        "                portfolio_feature_dim=portfolio_feature_dim,\n",
        "                latent_dim=latent_dim,\n",
        "                hidden_dim=hidden_dim,\n",
        "                num_heads=num_heads,\n",
        "                dropout_prob=dropout_prob,\n",
        "                num_inds=num_inds,\n",
        "                num_seeds=num_seeds,\n",
        "                use_isab=True,\n",
        "                num_layers=2\n",
        "            )\n",
        "        else:\n",
        "            self.branch_net = OptimizedSetTransformerEncoder(\n",
        "                portfolio_feature_dim=portfolio_feature_dim,\n",
        "                latent_dim=latent_dim,\n",
        "                hidden_dim=hidden_dim,\n",
        "                dropout_prob=dropout_prob,\n",
        "                num_heads=num_heads\n",
        "            )\n",
        "\n",
        "        # Trunk network\n",
        "        self.trunk_net = TrunkNet(\n",
        "            input_dim=1,\n",
        "            latent_dim=latent_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            dropout_prob=dropout_prob\n",
        "        )\n",
        "\n",
        "        # DeepONet parameters\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "        self.branch_scale = nn.Parameter(torch.ones(1) * 0.8)\n",
        "        self.trunk_scale = nn.Parameter(torch.ones(1) * 0.8)\n",
        "\n",
        "    def forward(self, portfolio, S_T):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            portfolio: [batch_size, portfolio_size, 3] - portfolio features\n",
        "            S_T: [batch_size, num_S_T_samples] - multiple S_T values per portfolio\n",
        "\n",
        "        Returns:\n",
        "            cashflows: [batch_size, num_S_T_samples] - predicted cashflows for each S_T\n",
        "        \"\"\"\n",
        "        # Branch network: encode portfolio\n",
        "        branch_out = self.branch_net(portfolio)  # [batch_size, latent_dim]\n",
        "        branch_out = branch_out * self.branch_scale\n",
        "\n",
        "        # Trunk network: process S_T values\n",
        "        trunk_out = self.trunk_net(S_T)  # [batch_size, num_S_T_samples, latent_dim]\n",
        "        trunk_out = trunk_out * self.trunk_scale\n",
        "\n",
        "        # Compute dot product: branch ⊗ trunk\n",
        "        branch_expanded = branch_out.unsqueeze(1)  # [batch_size, 1, latent_dim]\n",
        "\n",
        "        interaction = (branch_expanded * trunk_out).sum(dim=-1)  # [batch_size, num_S_T_samples]\n",
        "\n",
        "        # Add bias\n",
        "        cashflows = interaction\n",
        "        return cashflows"
      ],
      "metadata": {
        "id": "yEf2UaOm_-0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_stats(model):\n",
        "    \"\"\"Compute gradient statistics - fixed for single-element tensors\"\"\"\n",
        "    total_norm = 0\n",
        "    param_count = 0\n",
        "    gradient_stats = {}\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            param_norm = param.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "            param_count += 1\n",
        "\n",
        "            grad_data = param.grad.data\n",
        "            if grad_data.numel() > 1:\n",
        "                grad_std = grad_data.std(unbiased=False).item()\n",
        "            else:\n",
        "                grad_std = 0.0 # Handle single-element tensors\n",
        "\n",
        "            gradient_stats[name] = {\n",
        "                'norm': param_norm.item(),\n",
        "                'mean': grad_data.mean().item(),\n",
        "                'std': grad_std,\n",
        "                'max': grad_data.max().item(),\n",
        "                'min': grad_data.min().item(),\n",
        "                'shape': list(param.grad.shape),\n",
        "                'numel': grad_data.numel()\n",
        "            }\n",
        "\n",
        "    total_norm = total_norm ** (1. / 2)\n",
        "    return total_norm, gradient_stats, param_count\n",
        "\n",
        "def print_gradient_summary(gradient_stats, total_norm, epoch, batch_idx=None):\n",
        "    \"\"\"Enhanced gradient summary with more details\"\"\"\n",
        "    prefix = f\"Epoch {epoch}\" + (f\", Batch {batch_idx}\" if batch_idx is not None else \"\")\n",
        "    print(f\"\\n🔍 === Gradient Analysis - {prefix} ===\")\n",
        "    print(f\"Total Gradient Norm: {total_norm:.6f}\")\n",
        "\n",
        "    if total_norm > 30.0:\n",
        "        print(\"🚨 CRITICAL: Severe gradient explosion! Consider stopping training.\")\n",
        "    elif total_norm > 20.0:\n",
        "        print(\"⚠️  SEVERE: Major gradient explosion detected!\")\n",
        "    elif total_norm > 10.0:\n",
        "        print(\"⚠️  WARNING: Moderate gradient explosion detected!\")\n",
        "    elif total_norm < 1e-6:\n",
        "        print(\"⚠️  WARNING: Vanishing gradients detected!\")\n",
        "    else:\n",
        "        print(\"✅ Gradient norm is healthy\")\n",
        "\n",
        "    sorted_layers = sorted(gradient_stats.items(), key=lambda x: x[1]['norm'], reverse=True)\n",
        "    print(f\"\\nTop 5 layers by gradient norm (out of {len(gradient_stats)} total):\")\n",
        "    for i, (layer_name, stats) in enumerate(sorted_layers[:5]):\n",
        "        status = \"🔥\" if stats['norm'] > 3.0 else \"⚠️\" if stats['norm'] > 1.0 else \"✅\"\n",
        "        print(f\"  {status} {i+1}. {layer_name}: {stats['norm']:.4f}\")\n",
        "        print(f\"      Shape: {stats['shape']}, Elements: {stats['numel']}\")\n",
        "        print(f\"      Mean: {stats['mean']:.6f}, Std: {stats['std']:.6f}\")\n",
        "\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "vR_SMxGZAG6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "\n",
        "\n",
        "def rescale_derivative_autograd(pred_derivative_from_normalized_input, S_T_scalar_normalizer):\n",
        "\n",
        "    if S_T_scalar_normalizer is None:\n",
        "        # If no scaler was used, the derivative is already on the correct scale\n",
        "        return pred_derivative_from_normalized_input\n",
        "\n",
        "    # Ensure the scale factor is a torch tensor on the correct device\n",
        "    device = pred_derivative_from_normalized_input.device\n",
        "    dtype = pred_derivative_from_normalized_input.dtype\n",
        "\n",
        "    if hasattr(S_T_scalar_normalizer, 'data_max_') and hasattr(S_T_scalar_normalizer, 'data_min_'):\n",
        "        # Case: MinMaxScaler\n",
        "        # X_normalized = (X_original - min) / (max - min)\n",
        "        # d(X_normalized)/d(X_original) = 1 / (max - min)\n",
        "        # So, d(L)/d(X_original) = d(L)/d(X_normalized) * (1 / (max - min))\n",
        "        # Thus, d(L)/d(X_original) = d(L)/d(X_normalized) / (max - min)\n",
        "        # Or equivalently, d(L)/d(X_original) = d(L)/d(X_normalized) * (max - min) (your current implementation)\n",
        "        # This part of your code for MinMaxScaler is correct.\n",
        "\n",
        "        # Ensure data_range is a tensor\n",
        "        data_range = torch.tensor(S_T_scalar_normalizer.data_max_ - S_T_scalar_normalizer.data_min_,\n",
        "                                  dtype=dtype, device=device).squeeze() # Squeeze if it's (1,)\n",
        "\n",
        "        if data_range.item() < 1e-6: # Use .item() for scalar comparison\n",
        "            print(f\"⚠️ WARNING: S_T data_range is extremely small ({data_range.item():.4e}). Derivative may be unstable.\")\n",
        "            return torch.zeros_like(pred_derivative_from_normalized_input)\n",
        "\n",
        "        # MinMaxScaler: derivative must be multiplied by the range\n",
        "        return pred_derivative_from_normalized_input * data_range\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported scaler type: {type(S_T_scalar_normalizer)}. Expected StandardScaler.\")\n",
        "\n",
        "\n",
        "class ExtendedEarlyStopping:\n",
        "    # ... (no changes needed here) ...\n",
        "    def __init__(self, patience=30, min_delta=0.0005, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.best = float('inf')\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model=None):\n",
        "        if val_loss < self.best - self.min_delta:\n",
        "            self.best = val_loss\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "\n",
        "        if self.wait >= self.patience:\n",
        "            self.stopped_epoch = True\n",
        "            if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "\n",
        "        return self.stopped_epoch\n"
      ],
      "metadata": {
        "id": "tZQV7h_KAKlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stable_hyperparameters():\n",
        "    \"\"\"Return more stable hyperparameters\"\"\"\n",
        "    return {\n",
        "        \"learning_rate\": 1e-4,\n",
        "        \"weight_decay\": 1e-5 ,\n",
        "        \"lambda_deriv\": 0.2,\n",
        "        \"lambda_reg\": 1e-4,\n",
        "        \"gradient_clip_norm\": 5,\n",
        "        \"batch_size\": 128,\n",
        "        \"scheduler_T0\": 200,\n",
        "        \"early_stopping_patience\": 50,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "F8vpybx_AM8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "\n",
        "class OptimizedTrainer:\n",
        "    def __init__(self, model, device='cuda', monitor_gradients=True,\n",
        "                 scale_warmup_epochs=5, initial_scale=0.05, final_scale=1.0,\n",
        "                 learning_rate=5e-6, lambda_deriv_weight=0.1, weight_decay=1e-4, K_Scalar = None, ST_Scalar=None): # ADDED: learning_rate, lambda_deriv_weight, weight_decay for consistency\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.monitor_gradients = monitor_gradients\n",
        "        self.lambda_deriv_weight = lambda_deriv_weight # Store this for compute_loss\n",
        "\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate, # Use the passed learning_rate\n",
        "            weight_decay=weight_decay, # Use the passed weight_decay (or make it a constant here if not configurable)\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8\n",
        "        )\n",
        "\n",
        "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            self.optimizer, T_0=100, T_mult=2, eta_min=1e-6 # T_0 might also be a hyperparameter\n",
        "        )\n",
        "\n",
        "        self.scaler = GradScaler()\n",
        "        self.K_scalar = K_Scalar\n",
        "        self.S_T_scalar = ST_Scalar\n",
        "\n",
        "\n",
        "        self.cashflow_scaler = joblib.load(CASHFLOW_SCALAR_FILE) if os.path.exists(CASHFLOW_SCALAR_FILE) else None\n",
        "        if self.cashflow_scaler:\n",
        "          print(f\"LOADED CASHFLOW SCALARD FROM {CASHFLOW_SCALAR_FILE}\")\n",
        "\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.huber_loss = nn.SmoothL1Loss(beta=1.0)\n",
        "\n",
        "\n",
        "        # Scale warmup parameters\n",
        "        self.scale_warmup_epochs = scale_warmup_epochs\n",
        "        self.initial_scale = initial_scale\n",
        "        self.final_scale = final_scale\n",
        "\n",
        "        # Initialize model scales\n",
        "        if hasattr(self.model, 'branch_scale') and hasattr(self.model, 'trunk_scale'):\n",
        "            with torch.no_grad():\n",
        "                self.model.branch_scale.fill_(initial_scale)\n",
        "                self.model.trunk_scale.fill_(initial_scale)\n",
        "\n",
        "    def check_model_health(self, epoch, batch_idx):\n",
        "        \"\"\"Check model parameters for NaN/Inf values\"\"\"\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if torch.isnan(param).any():\n",
        "                print(f\"🚨 NaN parameter found: {name} at Epoch {epoch}, Batch {batch_idx}\")\n",
        "                return False\n",
        "            if torch.isinf(param).any():\n",
        "                print(f\"🚨 Inf parameter found: {name} at Epoch {epoch}, Batch {batch_idx}\")\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "\n",
        "    def compute_loss(self, pred_cashflow, true_cashflow, pred_deriv, true_deriv, lambda_reg=1e-4):\n",
        "\n",
        "\n",
        "      if self.cashflow_scaler is not None:\n",
        "\n",
        "          # pred_cashflow_rescaled = rescale_cashflow(pred_cashflow, self.cashflow_scaler)\n",
        "          # true_cashflow_rescaled = rescale_cashflow(true_cashflow, self.cashflow_scaler)\n",
        "          # cashflow_loss = self.huber_loss(pred_cashflow_rescaled, true_cashflow_rescaled)\n",
        "          cashflow_loss = self.huber_loss(pred_cashflow, true_cashflow)\n",
        "      else:\n",
        "          cashflow_loss = self.huber_loss(pred_cashflow, true_cashflow)\n",
        "      if true_deriv is not None:\n",
        "          pred_deriv_rescaled = rescale_derivative_autograd(pred_deriv, self.S_T_scalar)\n",
        "          deriv_loss = self.huber_loss(pred_deriv_rescaled, true_deriv)\n",
        "          total_loss = cashflow_loss + self.lambda_deriv_weight * deriv_loss\n",
        "      else:\n",
        "          deriv_loss = torch.tensor(0.0, device=pred_cashflow.device)\n",
        "          total_loss = cashflow_loss\n",
        "\n",
        "      return total_loss, cashflow_loss, deriv_loss\n",
        "\n",
        "\n",
        "    def train_step(self, portfolio, S_T, cashflow, true_derivative,\n",
        "               epoch=0, batch_idx=0, experiment=None):\n",
        "\n",
        "      # Check model health before training step\n",
        "      if not self.check_model_health(epoch, batch_idx):\n",
        "          return float('inf'), float('inf'), float('inf')\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      # Enable gradients for S_T\n",
        "      S_T = S_T.clone().detach().requires_grad_(True)\n",
        "\n",
        "      try:\n",
        "          pred_cashflow = self.model(portfolio, S_T)\n",
        "\n",
        "          # Check prediction health\n",
        "          if torch.isnan(pred_cashflow).any() or torch.isinf(pred_cashflow).any():\n",
        "              print(f\"🚨 Invalid predictions at Epoch {epoch}, Batch {batch_idx}\")\n",
        "              return float('inf'), float('inf'), float('inf')\n",
        "\n",
        "          # Compute derivatives\n",
        "          pred_deriv_from_autograd = None\n",
        "          if true_derivative is not None:\n",
        "              try:\n",
        "                  pred_deriv_from_autograd = torch.autograd.grad(\n",
        "                      outputs=pred_cashflow.sum(),\n",
        "                      inputs=S_T,\n",
        "                      retain_graph=True,\n",
        "                      create_graph=True,\n",
        "                      allow_unused=True\n",
        "                  )[0]\n",
        "\n",
        "                  if pred_deriv_from_autograd is not None:\n",
        "                      if torch.isnan(pred_deriv_from_autograd).any() or torch.isinf(pred_deriv_from_autograd).any():\n",
        "                          print(f\"🚨 Invalid derivatives at Epoch {epoch}, Batch {batch_idx}\")\n",
        "                          pred_deriv_from_autograd = None\n",
        "\n",
        "              except RuntimeError as e:\n",
        "                  print(f\"⚠️ Derivative computation failed: {e}\")\n",
        "                  pred_deriv_from_autograd = None\n",
        "\n",
        "          total_loss, cashflow_loss, deriv_loss = self.compute_loss(\n",
        "              pred_cashflow, cashflow, pred_deriv_from_autograd, true_derivative\n",
        "          )\n",
        "\n",
        "      except RuntimeError as e:\n",
        "          print(f\"⚠️ Forward pass failed at Epoch {epoch}, Batch {batch_idx}: {e}\")\n",
        "          return float('inf'), float('inf'), float('inf')\n",
        "\n",
        "      if torch.isnan(total_loss) or torch.isinf(total_loss):\n",
        "          print(f\"⚠️ Invalid total loss at Epoch {epoch}, Batch {batch_idx}\")\n",
        "          return float('inf'), float('inf'), float('inf')\n",
        "\n",
        "      # Backward pass\n",
        "      scaled_loss = self.scaler.scale(total_loss)\n",
        "      scaled_loss.backward()\n",
        "\n",
        "      self.scaler.unscale_(self.optimizer)\n",
        "\n",
        "      # Check gradients\n",
        "      gradient_health = True\n",
        "      max_grad_norm = 0.0\n",
        "      for name, param in self.model.named_parameters():\n",
        "          if param.grad is not None:\n",
        "              grad_norm = param.grad.norm().item()\n",
        "              max_grad_norm = max(max_grad_norm, grad_norm)\n",
        "\n",
        "              if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
        "                  print(f\"🚨 Bad gradient: {name} at Epoch {epoch}, Batch {batch_idx}\")\n",
        "                  gradient_health = False\n",
        "\n",
        "      if not gradient_health or max_grad_norm > 50.0:\n",
        "          print(f\"🛑 Unhealthy gradients. Max norm: {max_grad_norm:.4f}\")\n",
        "          self.scaler.update()\n",
        "          return float('inf'), float('inf'), float('inf')\n",
        "\n",
        "      # Clip gradients\n",
        "      total_norm_before = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1e10)\n",
        "      if total_norm_before > 5:\n",
        "          total_norm_after = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
        "          print(f\"Grad norm before: {total_norm_before:.2f}, after clipping: {total_norm_after:.2f}\")\n",
        "\n",
        "      # Log gradient stats\n",
        "      if self.monitor_gradients and (epoch % 5 == 0 and batch_idx % 2000 == 0):\n",
        "          total_norm, gradient_stats, _ = compute_gradient_stats(self.model)\n",
        "          print_gradient_summary(gradient_stats, total_norm, epoch, batch_idx)\n",
        "\n",
        "\n",
        "      # Optimizer step\n",
        "      self.scaler.step(self.optimizer)\n",
        "      self.scaler.update()\n",
        "      self.scheduler.step()\n",
        "\n",
        "      return total_loss.item(), cashflow_loss.item(), deriv_loss.item()\n",
        "\n",
        "\n",
        "\n",
        "    def val_step(self, portfolio, S_T, cashflow, derivative):\n",
        "        self.model.eval()\n",
        "        S_T.requires_grad_(True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.enable_grad():\n",
        "                pred_cashflow = self.model(portfolio, S_T)\n",
        "                pred_deriv_from_autograd = torch.autograd.grad(\n",
        "                    outputs=pred_cashflow,\n",
        "                    inputs=S_T,\n",
        "                    grad_outputs=torch.ones_like(pred_cashflow),\n",
        "                    retain_graph=False,\n",
        "                    create_graph=False\n",
        "                )[0]\n",
        "\n",
        "            val_total_loss, val_cashflow_loss, val_deriv_loss = self.compute_loss(\n",
        "                pred_cashflow, cashflow, pred_deriv_from_autograd, derivative\n",
        "            )\n",
        "\n",
        "        if S_T.requires_grad:\n",
        "            S_T.requires_grad_(False)\n",
        "\n",
        "        return val_total_loss.item(), val_cashflow_loss.item(), val_deriv_loss.item()\n",
        "\n",
        "\n",
        "    def update_scale(self, current_epoch):\n",
        "        # ... (no changes needed here) ...\n",
        "        if hasattr(self.model, 'branch_scale') and hasattr(self.model, 'trunk_scale'):\n",
        "            # Linear warmup from initial_scale to final_scale over scale_warmup_epochs\n",
        "            if current_epoch < self.scale_warmup_epochs:\n",
        "                factor = (current_epoch + 1) / self.scale_warmup_epochs\n",
        "                new_scale = self.initial_scale + (self.final_scale - self.initial_scale) * factor\n",
        "            else:\n",
        "                new_scale = self.final_scale\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.model.branch_scale.fill_(new_scale)\n",
        "                self.model.trunk_scale.fill_(new_scale)\n",
        "\n",
        "            print(f\"🔧 [Epoch {current_epoch}] Updated branch/trunk scale → {new_scale:.4f}\")\n"
      ],
      "metadata": {
        "id": "2Jx4Xu-nAPH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment.end()"
      ],
      "metadata": {
        "id": "dWSulG29A67D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_dim = 64\n",
        "latent_dim = 64\n",
        "batch_size = 128\n",
        "\n",
        "portfolio_feature_dim = 3\n",
        "\n",
        "PORT_LEN = 50\n",
        "PORT_SAMPLE_SIZE = 10000\n",
        "FEED_ST_LEN_EACH_PORT = 150\n",
        "\n",
        "final_save_path = '/content/drive/MyDrive/Ucl/'\n",
        "model_path = final_save_path + 'final_deeponet_model.pt'\n",
        "\n",
        "raw_dataset = OperatorDatasetStandarization(\n",
        "      num_samples=PORT_SAMPLE_SIZE,\n",
        "      portfolio_size=PORT_LEN,\n",
        "      num_samples_S_T=FEED_ST_LEN_EACH_PORT,\n",
        "      is_fitting_mode=True  # No normalization during fitting\n",
        "      )\n",
        "\n",
        "raw_data_loader = DataLoader(raw_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "K_scalar, S_T_scalar = fit_K_ST_scalers(raw_data_loader)\n",
        "cashflow_scaler = fit_cashflow_scaler(raw_data_loader)\n",
        "\n",
        "normalized_dataset = OperatorDatasetStandarization(\n",
        "        num_samples=PORT_SAMPLE_SIZE,\n",
        "        portfolio_size=PORT_LEN,\n",
        "        num_samples_S_T=FEED_ST_LEN_EACH_PORT,\n",
        "        K_Scalar=K_scalar,\n",
        "        S_T_scalar=S_T_scalar,\n",
        "        cashflow_scaler=cashflow_scaler,\n",
        "        is_fitting_mode=False)\n",
        "\n",
        "# --- Load the trained model ---\n",
        "deeponet_model = OptimizedDeepONet(\n",
        "    portfolio_feature_dim=portfolio_feature_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    latent_dim=latent_dim,\n",
        "    use_enhanced_transformer= True\n",
        ").to(DEVICE)\n",
        "\n",
        "try:\n",
        "    deeponet_model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
        "    print(f\"Successfully loaded model from {model_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Model file not found at {model_path}. Please ensure the model is trained and saved.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    exit()\n",
        "\n",
        "deeponet_model.eval()  # Set model to evaluation mode\n",
        "\n",
        "eval_dataloader = DataLoader(normalized_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Get a sample for plotting\n",
        "with torch.no_grad():\n",
        "    for i, (portfolio, S_T_i_normalized, true_cashflows_normalized, derivative) in enumerate(eval_dataloader):\n",
        "\n",
        "        # Make prediction\n",
        "        predicted_cashflows_normalized = deeponet_model(portfolio, S_T_i_normalized)\n",
        "\n",
        "        # Squeeze the batch dimension\n",
        "        predicted_cashflows_normalized = predicted_cashflows_normalized.squeeze(0)\n",
        "        true_cashflows_normalized = true_cashflows_normalized.squeeze(0)\n",
        "        S_T_i_normalized = S_T_i_normalized.squeeze(0)\n",
        "\n",
        "        # 1. CHECK LOSS CALCULATION IN NORMALIZED SPACE\n",
        "        mse_normalized = torch.mean((true_cashflows_normalized - predicted_cashflows_normalized) ** 2)\n",
        "\n",
        "        # 2. ANALYZE NORMALIZED VALUES\n",
        "        # Convert to numpy for scaler operations\n",
        "        S_T_i_numpy = S_T_i_normalized.cpu().numpy()\n",
        "        true_cashflows_numpy = true_cashflows_normalized.cpu().numpy()\n",
        "        predicted_cashflows_numpy = predicted_cashflows_normalized.cpu().numpy()\n",
        "\n",
        "        # 3. DENORMALIZE\n",
        "        s_t_values_denormalized = S_T_scalar.inverse_transform(\n",
        "            S_T_i_numpy.reshape(-1, 1)\n",
        "        ).squeeze()\n",
        "\n",
        "        true_cashflows_denormalized = cashflow_scaler.inverse_transform(\n",
        "            true_cashflows_numpy.reshape(-1, 1)\n",
        "        ).squeeze()\n",
        "\n",
        "        predicted_cashflows_denormalized = cashflow_scaler.inverse_transform(\n",
        "            predicted_cashflows_numpy.reshape(-1, 1)\n",
        "        ).squeeze()\n",
        "\n",
        "        # 4. DENORMALIZED ERROR ANALYSIS\n",
        "        mse_denormalized = mean_squared_error(true_cashflows_denormalized, predicted_cashflows_denormalized)\n",
        "        rmse_denormalized = np.sqrt(mse_denormalized)\n",
        "\n",
        "        # 5. SORT DATA FOR PLOTTING\n",
        "        # For normalized plots\n",
        "        sorted_indices_norm = np.argsort(S_T_i_numpy)\n",
        "        s_t_values_norm_sorted = S_T_i_numpy[sorted_indices_norm]\n",
        "        true_cashflows_norm_sorted = true_cashflows_numpy[sorted_indices_norm]\n",
        "        predicted_cashflows_norm_sorted = predicted_cashflows_numpy[sorted_indices_norm]\n",
        "\n",
        "        # For denormalized plots\n",
        "        sorted_indices = np.argsort(s_t_values_denormalized)\n",
        "        s_t_values_sorted = s_t_values_denormalized[sorted_indices]\n",
        "        true_cashflows_sorted = true_cashflows_denormalized[sorted_indices]\n",
        "        predicted_cashflows_sorted = predicted_cashflows_denormalized[sorted_indices]\n",
        "\n",
        "        # 6. COMPREHENSIVE PLOTTING - NOW WITH NORMALIZED AND DENORMALIZED\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "        # ===== NORMALIZED SPACE PLOTS =====\n",
        "        # Plot 1: Normalized comparison\n",
        "        axes[0, 0].plot(s_t_values_norm_sorted, true_cashflows_norm_sorted,\n",
        "                       label='True', color='blue', marker='o', markersize=3, linewidth=2)\n",
        "        axes[0, 0].plot(s_t_values_norm_sorted, predicted_cashflows_norm_sorted,\n",
        "                       label='Predicted', color='red', linestyle='--', marker='x', markersize=3, linewidth=2)\n",
        "        axes[0, 0].set_xlabel('S_T (Normalized)')\n",
        "        axes[0, 0].set_ylabel('Cashflow (Normalized)')\n",
        "        axes[0, 0].set_title(f'Cashflow Prediction\\nRMSE: {rmse_denormalized:.6f}')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Normalized residuals\n",
        "        residuals_norm = true_cashflows_norm_sorted - predicted_cashflows_norm_sorted\n",
        "        axes[1, 0].scatter(s_t_values_norm_sorted, residuals_norm, alpha=0.6, color='green', s=20)\n",
        "        axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "        axes[1, 0].set_xlabel('S_T (Normalized)')\n",
        "        axes[1, 0].set_ylabel('Residuals (Normalized)')\n",
        "        axes[1, 0].set_title(f'Cashflow Residuals\\nMAE: {np.mean(np.abs(residuals_norm)):.6f}')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # ===== DENORMALIZED SPACE PLOTS =====\n",
        "        # Plot 3: Denormalized comparison\n",
        "        axes[0, 1].plot(s_t_values_sorted, true_cashflows_sorted,\n",
        "                       label='True', color='blue', marker='o', markersize=3, linewidth=2)\n",
        "        axes[0, 1].plot(s_t_values_sorted, predicted_cashflows_sorted,\n",
        "                       label='Predicted', color='red', linestyle='--', marker='x', markersize=3, linewidth=2)\n",
        "        axes[0, 1].set_xlabel('S_T (Denormalized)')\n",
        "        axes[0, 1].set_ylabel('Cashflow (Denormalized)')\n",
        "        axes[0, 1].set_title(f'Cashflow Prediction Comparison\\nRMSE: {rmse_denormalized:.6f}')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 4: Denormalized residuals\n",
        "        residuals = true_cashflows_sorted - predicted_cashflows_sorted\n",
        "        axes[1, 1].scatter(s_t_values_sorted, residuals, alpha=0.6, color='green', s=20)\n",
        "        axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "        axes[1, 1].set_xlabel('S_T (Denormalized)')\n",
        "        axes[1, 1].set_ylabel('Residuals (Denormalized)')\n",
        "        axes[1, 1].set_title(f'Cashflow Residuals\\nMAE: {np.mean(np.abs(residuals)):.6f}')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Break after first sample\n",
        "        if i == 0:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "hlhMf78VARuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KwCKAgZqA2Jr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}